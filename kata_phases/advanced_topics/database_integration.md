# Database Integration: ä¼æ¥­ç´šè³‡æ–™åº«æ¶æ§‹èˆ‡æœ€ä½³å¯¦å‹™

## ğŸ¯ å­¸ç¿’ç›®æ¨™
- æŒæ¡ç¾ä»£è³‡æ–™åº«æ¶æ§‹è¨­è¨ˆæ¨¡å¼
- å­¸ç¿’ SQLAlchemy é€²éšç‰¹æ€§èˆ‡æ•ˆèƒ½å„ªåŒ–
- å»ºç«‹è³‡æ–™åº«é·ç§»èˆ‡ç‰ˆæœ¬æ§åˆ¶ç­–ç•¥
- å¯¦è¸è³‡æ–™åº«ç›£æ§èˆ‡æ•ˆèƒ½èª¿å„ª

---

## ğŸ—„ï¸ SQLAlchemy é€²éšæ¶æ§‹è¨­è¨ˆ

### ä¼æ¥­ç´šè³‡æ–™åº«é€£æ¥ç®¡ç†

```python
# app/database/connection.py
from sqlalchemy.ext.asyncio import (
    AsyncSession,
    create_async_engine,
    AsyncConnection,
    async_sessionmaker
)
from sqlalchemy.pool import QueuePool, NullPool
from sqlalchemy.engine.events import event
from typing import AsyncGenerator
import structlog

logger = structlog.get_logger()

class DatabaseManager:
    """ä¼æ¥­ç´šè³‡æ–™åº«ç®¡ç†å™¨"""

    def __init__(self, config: DatabaseConfig):
        self.config = config
        self.engine = self._create_engine()
        self.session_factory = self._create_session_factory()
        self._setup_event_listeners()

    def _create_engine(self):
        """å»ºç«‹å„ªåŒ–çš„è³‡æ–™åº«å¼•æ“"""
        engine_kwargs = {
            "echo": self.config.debug,
            "echo_pool": self.config.debug_pool,
            "future": True,  # ä½¿ç”¨ SQLAlchemy 2.0 æ¨£å¼
            "pool_size": self.config.pool_size,
            "max_overflow": self.config.max_overflow,
            "pool_timeout": self.config.pool_timeout,
            "pool_recycle": self.config.pool_recycle,
            "pool_pre_ping": True,  # é€£æ¥å¥åº·æª¢æŸ¥
        }

        # æ ¹æ“šç’°å¢ƒé¸æ“‡é€£æ¥æ± 
        if self.config.environment == "test":
            engine_kwargs["poolclass"] = NullPool  # æ¸¬è©¦ç’°å¢ƒä¸ä½¿ç”¨é€£æ¥æ± 
        else:
            engine_kwargs["poolclass"] = QueuePool

        return create_async_engine(self.config.url, **engine_kwargs)

    def _create_session_factory(self):
        """å»ºç«‹æœƒè©±å·¥å» """
        return async_sessionmaker(
            self.engine,
            class_=AsyncSession,
            autoflush=False,  # æ‰‹å‹•æ§åˆ¶ flush æ™‚æ©Ÿ
            autocommit=False,
            expire_on_commit=False  # é¿å…æ‡¶è¼‰å…¥å•é¡Œ
        )

    def _setup_event_listeners(self):
        """è¨­å®šè³‡æ–™åº«äº‹ä»¶ç›£è½å™¨"""

        @event.listens_for(self.engine.sync_engine, "connect")
        def set_sqlite_pragma(dbapi_connection, connection_record):
            """SQLite å°ˆç”¨è¨­å®š"""
            if "sqlite" in str(self.engine.url):
                cursor = dbapi_connection.cursor()
                cursor.execute("PRAGMA foreign_keys=ON")
                cursor.execute("PRAGMA journal_mode=WAL")
                cursor.close()

        @event.listens_for(self.engine.sync_engine, "checkout")
        def receive_checkout(dbapi_connection, connection_record, connection_proxy):
            """é€£æ¥å–å‡ºæ™‚çš„è™•ç†"""
            logger.debug("Database connection checked out",
                        connection_id=id(dbapi_connection))

        @event.listens_for(self.engine.sync_engine, "checkin")
        def receive_checkin(dbapi_connection, connection_record):
            """é€£æ¥æ­¸é‚„æ™‚çš„è™•ç†"""
            logger.debug("Database connection checked in",
                        connection_id=id(dbapi_connection))

    async def get_session(self) -> AsyncGenerator[AsyncSession, None]:
        """å–å¾—è³‡æ–™åº«æœƒè©±"""
        async with self.session_factory() as session:
            try:
                yield session
            except Exception:
                await session.rollback()
                raise
            finally:
                await session.close()

    async def health_check(self) -> bool:
        """è³‡æ–™åº«å¥åº·æª¢æŸ¥"""
        try:
            async with self.engine.begin() as conn:
                await conn.execute(text("SELECT 1"))
            return True
        except Exception as e:
            logger.error("Database health check failed", error=str(e))
            return False

    async def get_connection_stats(self) -> dict:
        """å–å¾—é€£æ¥æ± çµ±è¨ˆ"""
        pool = self.engine.pool
        return {
            "pool_size": pool.size(),
            "checked_in": pool.checkedin(),
            "checked_out": pool.checkedout(),
            "overflow": pool.overflow(),
            "invalid": pool.invalid()
        }

    async def close(self):
        """é—œé–‰è³‡æ–™åº«å¼•æ“"""
        await self.engine.dispose()

class ReadWriteSplitManager:
    """è®€å¯«åˆ†é›¢ç®¡ç†å™¨"""

    def __init__(self, write_config: DatabaseConfig, read_configs: List[DatabaseConfig]):
        self.write_db = DatabaseManager(write_config)
        self.read_dbs = [DatabaseManager(config) for config in read_configs]
        self.read_db_index = 0

    def get_read_db(self) -> DatabaseManager:
        """å–å¾—è®€å–è³‡æ–™åº«ï¼ˆè² è¼‰å‡è¡¡ï¼‰"""
        db = self.read_dbs[self.read_db_index]
        self.read_db_index = (self.read_db_index + 1) % len(self.read_dbs)
        return db

    def get_write_db(self) -> DatabaseManager:
        """å–å¾—å¯«å…¥è³‡æ–™åº«"""
        return self.write_db

    async def transaction_context(self, read_only: bool = False):
        """äº¤æ˜“ä¸Šä¸‹æ–‡ç®¡ç†"""
        db = self.get_read_db() if read_only else self.get_write_db()
        async with db.get_session() as session:
            yield session
```

### é€²éšæ¨¡å‹è¨­è¨ˆæ¨¡å¼

```python
# app/models/base.py
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.ext.hybrid import hybrid_property
from sqlalchemy import Column, DateTime, String, Boolean
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy.sql import func
from datetime import datetime
import uuid

Base = declarative_base()

class TimestampMixin:
    """æ™‚é–“æˆ³æ··å…¥é¡åˆ¥"""
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False)

class SoftDeleteMixin:
    """è»Ÿåˆªé™¤æ··å…¥é¡åˆ¥"""
    deleted_at = Column(DateTime(timezone=True), nullable=True)
    is_deleted = Column(Boolean, default=False, nullable=False)

    @hybrid_property
    def is_active(self):
        return not self.is_deleted

    def soft_delete(self):
        self.is_deleted = True
        self.deleted_at = datetime.utcnow()

class AuditMixin:
    """å¯©è¨ˆæ··å…¥é¡åˆ¥"""
    created_by = Column(String(255), nullable=True)
    updated_by = Column(String(255), nullable=True)
    version = Column(Integer, default=1, nullable=False)

    def increment_version(self):
        self.version += 1

class BaseModel(Base, TimestampMixin, SoftDeleteMixin, AuditMixin):
    """åŸºç¤æ¨¡å‹é¡åˆ¥"""
    __abstract__ = True

    id = Column(UUID(as_uuid=True), primary_key=True, default=uuid.uuid4)

    def to_dict(self) -> dict:
        """è½‰æ›ç‚ºå­—å…¸"""
        return {c.name: getattr(self, c.name) for c in self.__table__.columns}

    def __repr__(self):
        return f"<{self.__class__.__name__}(id={self.id})>"

# app/models/user.py
from sqlalchemy import Column, String, Boolean, Integer, Index, UniqueConstraint
from sqlalchemy.orm import relationship
from sqlalchemy.ext.hybrid import hybrid_property
from .base import BaseModel

class User(BaseModel):
    """ä½¿ç”¨è€…æ¨¡å‹"""
    __tablename__ = "users"

    # åŸºæœ¬æ¬„ä½
    username = Column(String(50), unique=True, nullable=False)
    email = Column(String(255), unique=True, nullable=False)
    hashed_password = Column(String(255), nullable=False)

    # å€‹äººè³‡è¨Š
    first_name = Column(String(100), nullable=True)
    last_name = Column(String(100), nullable=True)
    display_name = Column(String(200), nullable=True)

    # å¸³æˆ¶ç‹€æ…‹
    is_active = Column(Boolean, default=True, nullable=False)
    is_verified = Column(Boolean, default=False, nullable=False)
    email_verified_at = Column(DateTime(timezone=True), nullable=True)

    # å®‰å…¨æ¬„ä½
    failed_login_attempts = Column(Integer, default=0, nullable=False)
    locked_until = Column(DateTime(timezone=True), nullable=True)
    password_changed_at = Column(DateTime(timezone=True), nullable=True)
    last_login_at = Column(DateTime(timezone=True), nullable=True)
    last_login_ip = Column(String(45), nullable=True)  # IPv6 æ”¯æ´

    # MFA è¨­å®š
    mfa_enabled = Column(Boolean, default=False, nullable=False)
    mfa_secret = Column(String(255), nullable=True)
    backup_codes = Column(JSON, nullable=True)

    # é—œè¯
    user_sessions = relationship("UserSession", back_populates="user", cascade="all, delete-orphan")
    login_history = relationship("LoginHistory", back_populates="user", cascade="all, delete-orphan")

    # ç´¢å¼•
    __table_args__ = (
        Index('ix_users_username_active', 'username', 'is_active'),
        Index('ix_users_email_active', 'email', 'is_active'),
        Index('ix_users_created_at', 'created_at'),
        UniqueConstraint('email', name='uq_users_email'),
        UniqueConstraint('username', name='uq_users_username'),
    )

    @hybrid_property
    def full_name(self):
        """å®Œæ•´å§“å"""
        if self.first_name and self.last_name:
            return f"{self.first_name} {self.last_name}"
        return self.display_name or self.username

    @hybrid_property
    def is_locked(self):
        """æª¢æŸ¥å¸³æˆ¶æ˜¯å¦è¢«é–å®š"""
        if self.locked_until:
            return datetime.utcnow() < self.locked_until
        return False

    def lock_account(self, duration_minutes: int = 30):
        """é–å®šå¸³æˆ¶"""
        self.locked_until = datetime.utcnow() + timedelta(minutes=duration_minutes)

    def unlock_account(self):
        """è§£é–å¸³æˆ¶"""
        self.locked_until = None
        self.failed_login_attempts = 0

    def record_failed_login(self):
        """è¨˜éŒ„ç™»å…¥å¤±æ•—"""
        self.failed_login_attempts += 1
        if self.failed_login_attempts >= 5:  # 5æ¬¡å¤±æ•—å¾Œé–å®š
            self.lock_account()

    def record_successful_login(self, ip_address: str):
        """è¨˜éŒ„æˆåŠŸç™»å…¥"""
        self.last_login_at = datetime.utcnow()
        self.last_login_ip = ip_address
        self.failed_login_attempts = 0
        self.locked_until = None

class UserSession(BaseModel):
    """ä½¿ç”¨è€…æœƒè©±æ¨¡å‹"""
    __tablename__ = "user_sessions"

    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"), nullable=False)
    session_token = Column(String(255), unique=True, nullable=False)
    refresh_token = Column(String(255), unique=True, nullable=True)

    expires_at = Column(DateTime(timezone=True), nullable=False)
    last_activity = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)

    # æœƒè©±è³‡è¨Š
    ip_address = Column(String(45), nullable=True)
    user_agent = Column(String(500), nullable=True)
    device_fingerprint = Column(String(255), nullable=True)

    # é—œè¯
    user = relationship("User", back_populates="user_sessions")

    # ç´¢å¼•
    __table_args__ = (
        Index('ix_sessions_user_id', 'user_id'),
        Index('ix_sessions_expires_at', 'expires_at'),
        Index('ix_sessions_token', 'session_token'),
    )

    @hybrid_property
    def is_expired(self):
        """æª¢æŸ¥æœƒè©±æ˜¯å¦éæœŸ"""
        return datetime.utcnow() > self.expires_at

    @hybrid_property
    def is_active(self):
        """æª¢æŸ¥æœƒè©±æ˜¯å¦æ´»èº"""
        return not self.is_expired and not self.is_deleted

    def extend_session(self, hours: int = 24):
        """å»¶é•·æœƒè©±"""
        self.expires_at = datetime.utcnow() + timedelta(hours=hours)
        self.last_activity = datetime.utcnow()

class LoginHistory(BaseModel):
    """ç™»å…¥æ­·å²è¨˜éŒ„"""
    __tablename__ = "login_history"

    user_id = Column(UUID(as_uuid=True), ForeignKey("users.id"), nullable=False)
    ip_address = Column(String(45), nullable=False)
    user_agent = Column(String(500), nullable=True)
    location = Column(String(255), nullable=True)  # åœ°ç†ä½ç½®
    success = Column(Boolean, nullable=False)
    failure_reason = Column(String(255), nullable=True)

    # é¢¨éšªè©•åˆ†
    risk_score = Column(Integer, default=0, nullable=False)
    unusual_activity = Column(Boolean, default=False, nullable=False)

    # é—œè¯
    user = relationship("User", back_populates="login_history")

    # ç´¢å¼•
    __table_args__ = (
        Index('ix_login_history_user_id', 'user_id'),
        Index('ix_login_history_created_at', 'created_at'),
        Index('ix_login_history_ip', 'ip_address'),
        Index('ix_login_history_success', 'success'),
    )
```

---

## ğŸ”„ è³‡æ–™åº«é·ç§»èˆ‡ç‰ˆæœ¬æ§åˆ¶

### Alembic é€²éšé·ç§»ç­–ç•¥

```python
# alembic/env.py
from logging.config import fileConfig
from sqlalchemy import engine_from_config, pool
from alembic import context
from app.models import Base
import os

# Alembic Config object
config = context.config

# è¨­å®šè³‡æ–™åº« URL
database_url = os.environ.get("DATABASE_URL")
if database_url:
    config.set_main_option("sqlalchemy.url", database_url)

# Interpret the config file for Python logging
if config.config_file_name is not None:
    fileConfig(config.config_file_name)

# ç›®æ¨™ metadata
target_metadata = Base.metadata

def include_object(object, name, type_, reflected, compare_to):
    """éæ¿¾è¦åŒ…å«åœ¨é·ç§»ä¸­çš„ç‰©ä»¶"""
    # æ’é™¤è‡¨æ™‚è¡¨
    if type_ == "table" and name.startswith("temp_"):
        return False

    # æ’é™¤è¦–åœ–
    if type_ == "table" and reflected and object.info.get("is_view"):
        return False

    return True

def run_migrations_offline():
    """é›¢ç·šæ¨¡å¼é·ç§»"""
    url = config.get_main_option("sqlalchemy.url")
    context.configure(
        url=url,
        target_metadata=target_metadata,
        literal_binds=True,
        dialect_opts={"paramstyle": "named"},
        include_object=include_object,
        compare_type=True,
        compare_server_default=True,
    )

    with context.begin_transaction():
        context.run_migrations()

def run_migrations_online():
    """ç·šä¸Šæ¨¡å¼é·ç§»"""
    connectable = engine_from_config(
        config.get_section(config.config_ini_section),
        prefix="sqlalchemy.",
        poolclass=pool.NullPool,
    )

    with connectable.connect() as connection:
        context.configure(
            connection=connection,
            target_metadata=target_metadata,
            include_object=include_object,
            compare_type=True,
            compare_server_default=True,
        )

        with context.begin_transaction():
            context.run_migrations()

# åˆ¤æ–·åŸ·è¡Œæ¨¡å¼
if context.is_offline_mode():
    run_migrations_offline()
else:
    run_migrations_online()

# scripts/migration_manager.py
class MigrationManager:
    """é·ç§»ç®¡ç†å™¨"""

    def __init__(self, config: AlembicConfig):
        self.config = config

    async def create_migration(self, message: str, auto_generate: bool = True) -> str:
        """å»ºç«‹æ–°çš„é·ç§»æª”æ¡ˆ"""
        cmd_opts = []
        if auto_generate:
            cmd_opts.append("--autogenerate")

        cmd_opts.extend(["-m", message])

        # åŸ·è¡Œ alembic revision
        revision_id = alembic.command.revision(self.config, message, autogenerate=auto_generate)

        # é·ç§»æª”æ¡ˆå¾Œè™•ç†
        await self._post_process_migration(revision_id)

        return revision_id

    async def _post_process_migration(self, revision_id: str):
        """é·ç§»æª”æ¡ˆå¾Œè™•ç†"""
        migration_file = self._get_migration_file(revision_id)

        # æ·»åŠ å®‰å…¨æª¢æŸ¥
        await self._add_safety_checks(migration_file)

        # é©—è­‰é·ç§»èªæ³•
        await self._validate_migration_syntax(migration_file)

        # ä¼°ç®—é·ç§»æ™‚é–“
        estimated_time = await self._estimate_migration_time(migration_file)

        logger.info(f"Migration {revision_id} created",
                   estimated_time=estimated_time)

    async def apply_migrations(self, target_revision: str = "head", dry_run: bool = False):
        """æ‡‰ç”¨é·ç§»"""
        if dry_run:
            # æª¢æŸ¥é·ç§»è¨ˆåŠƒ
            current = alembic.command.current(self.config)
            plan = alembic.command.show(self.config, target_revision)

            logger.info("Migration plan", current=current, target=target_revision, plan=plan)
            return

        # å‚™ä»½è³‡æ–™åº«ï¼ˆç”Ÿç”¢ç’°å¢ƒï¼‰
        if self._is_production():
            backup_id = await self._backup_database()
            logger.info("Database backed up", backup_id=backup_id)

        try:
            # åŸ·è¡Œé·ç§»
            alembic.command.upgrade(self.config, target_revision)
            logger.info("Migration applied successfully", target=target_revision)

        except Exception as e:
            if self._is_production():
                # å›å¾©å‚™ä»½
                await self._restore_database(backup_id)
            raise MigrationError(f"Migration failed: {str(e)}")

    async def rollback_migration(self, target_revision: str):
        """å›æ»¾é·ç§»"""
        # æª¢æŸ¥å›æ»¾å®‰å…¨æ€§
        if not await self._is_safe_to_rollback(target_revision):
            raise MigrationError("Rollback not safe - data loss may occur")

        # å‚™ä»½ç•¶å‰ç‹€æ…‹
        backup_id = await self._backup_database()

        try:
            alembic.command.downgrade(self.config, target_revision)
            logger.info("Migration rolled back", target=target_revision)

        except Exception as e:
            await self._restore_database(backup_id)
            raise MigrationError(f"Rollback failed: {str(e)}")

class ZeroDowntimeMigration:
    """é›¶åœæ©Ÿæ™‚é–“é·ç§»"""

    async def add_column_safely(self, table_name: str, column_def: dict):
        """å®‰å…¨åœ°æ·»åŠ æ¬„ä½"""
        steps = [
            # 1. æ·»åŠ æ¬„ä½ï¼ˆå…è¨± NULLï¼‰
            f"ALTER TABLE {table_name} ADD COLUMN {column_def['name']} {column_def['type']} NULL",

            # 2. å¡«å……é è¨­å€¼ï¼ˆåˆ†æ‰¹è™•ç†ï¼‰
            self._create_backfill_script(table_name, column_def),

            # 3. æ·»åŠ éç©ºç´„æŸï¼ˆå¦‚æœéœ€è¦ï¼‰
            f"ALTER TABLE {table_name} ALTER COLUMN {column_def['name']} SET NOT NULL" if column_def.get('not_null') else None,

            # 4. æ·»åŠ ç´¢å¼•ï¼ˆä¸¦ç™¼å»ºç«‹ï¼‰
            f"CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_{table_name}_{column_def['name']} ON {table_name} ({column_def['name']})" if column_def.get('indexed') else None
        ]

        for step in filter(None, steps):
            await self._execute_with_retry(step)

    async def remove_column_safely(self, table_name: str, column_name: str):
        """å®‰å…¨åœ°ç§»é™¤æ¬„ä½"""
        steps = [
            # 1. ç§»é™¤ç´¢å¼•
            f"DROP INDEX IF EXISTS idx_{table_name}_{column_name}",

            # 2. ç§»é™¤ç´„æŸ
            f"ALTER TABLE {table_name} ALTER COLUMN {column_name} DROP NOT NULL",

            # 3. ç­‰å¾…ç¢ºèªæ²’æœ‰æ‡‰ç”¨ç¨‹å¼ä½¿ç”¨æ­¤æ¬„ä½
            self._wait_for_deployment_confirmation(),

            # 4. ç§»é™¤æ¬„ä½
            f"ALTER TABLE {table_name} DROP COLUMN {column_name}"
        ]

        for step in steps:
            await self._execute_with_retry(step)

    def _create_backfill_script(self, table_name: str, column_def: dict) -> str:
        """å»ºç«‹å›å¡«è…³æœ¬"""
        batch_size = 10000
        default_value = column_def.get('default', 'NULL')

        return f"""
        DO $$
        DECLARE
            batch_start INTEGER := 0;
            batch_end INTEGER := {batch_size};
            affected_rows INTEGER;
        BEGIN
            LOOP
                UPDATE {table_name}
                SET {column_def['name']} = {default_value}
                WHERE id IN (
                    SELECT id FROM {table_name}
                    WHERE {column_def['name']} IS NULL
                    ORDER BY id
                    LIMIT {batch_size}
                );

                GET DIAGNOSTICS affected_rows = ROW_COUNT;
                EXIT WHEN affected_rows = 0;

                -- æš«åœä»¥é¿å…é–å®šéä¹…
                PERFORM pg_sleep(0.1);
            END LOOP;
        END
        $$;
        """
```

---

## ğŸ“Š è³‡æ–™åº«æ•ˆèƒ½å„ªåŒ–

### æŸ¥è©¢å„ªåŒ–èˆ‡ç´¢å¼•ç­–ç•¥

```python
# app/database/query_optimizer.py
from sqlalchemy import text, event
from sqlalchemy.engine import Engine
import time
import structlog

logger = structlog.get_logger()

class QueryPerformanceMonitor:
    """æŸ¥è©¢æ•ˆèƒ½ç›£æ§å™¨"""

    def __init__(self, slow_query_threshold: float = 1.0):
        self.slow_query_threshold = slow_query_threshold
        self.query_stats = defaultdict(lambda: {"count": 0, "total_time": 0, "avg_time": 0})

    def setup_monitoring(self, engine: Engine):
        """è¨­å®šæŸ¥è©¢ç›£æ§"""

        @event.listens_for(engine, "before_cursor_execute")
        def receive_before_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            context._query_start_time = time.time()

        @event.listens_for(engine, "after_cursor_execute")
        def receive_after_cursor_execute(conn, cursor, statement, parameters, context, executemany):
            total = time.time() - context._query_start_time

            # è¨˜éŒ„çµ±è¨ˆ
            query_hash = hash(statement)
            stats = self.query_stats[query_hash]
            stats["count"] += 1
            stats["total_time"] += total
            stats["avg_time"] = stats["total_time"] / stats["count"]

            # è¨˜éŒ„æ…¢æŸ¥è©¢
            if total > self.slow_query_threshold:
                logger.warning("Slow query detected",
                             duration=total,
                             statement=statement[:200],
                             parameters=parameters)

    def get_performance_report(self) -> dict:
        """å–å¾—æ•ˆèƒ½å ±å‘Š"""
        slow_queries = []
        for query_hash, stats in self.query_stats.items():
            if stats["avg_time"] > self.slow_query_threshold:
                slow_queries.append({
                    "query_hash": query_hash,
                    "avg_time": stats["avg_time"],
                    "count": stats["count"],
                    "total_time": stats["total_time"]
                })

        return {
            "total_queries": sum(stats["count"] for stats in self.query_stats.values()),
            "slow_queries": sorted(slow_queries, key=lambda x: x["avg_time"], reverse=True),
            "avg_query_time": sum(stats["avg_time"] for stats in self.query_stats.values()) / len(self.query_stats)
        }

class IndexAnalyzer:
    """ç´¢å¼•åˆ†æå™¨"""

    def __init__(self, session: AsyncSession):
        self.session = session

    async def analyze_missing_indexes(self) -> List[dict]:
        """åˆ†æç¼ºå¤±çš„ç´¢å¼•"""
        # PostgreSQL å°ˆç”¨æŸ¥è©¢
        query = text("""
        SELECT
            schemaname,
            tablename,
            attname,
            n_distinct,
            correlation,
            most_common_vals
        FROM pg_stats
        WHERE schemaname = 'public'
        AND n_distinct > 100
        AND correlation < 0.1
        ORDER BY n_distinct DESC;
        """)

        result = await self.session.execute(query)
        return [dict(row) for row in result.fetchall()]

    async def analyze_unused_indexes(self) -> List[dict]:
        """åˆ†ææœªä½¿ç”¨çš„ç´¢å¼•"""
        query = text("""
        SELECT
            schemaname,
            tablename,
            indexname,
            idx_tup_read,
            idx_tup_fetch,
            pg_size_pretty(pg_relation_size(indexrelid)) as size
        FROM pg_stat_user_indexes
        WHERE idx_tup_read = 0
        ORDER BY pg_relation_size(indexrelid) DESC;
        """)

        result = await self.session.execute(query)
        return [dict(row) for row in result.fetchall()]

    async def get_index_usage_stats(self) -> List[dict]:
        """å–å¾—ç´¢å¼•ä½¿ç”¨çµ±è¨ˆ"""
        query = text("""
        SELECT
            schemaname,
            tablename,
            indexname,
            idx_tup_read,
            idx_tup_fetch,
            idx_scan,
            pg_size_pretty(pg_relation_size(indexrelid)) as size
        FROM pg_stat_user_indexes
        ORDER BY idx_scan DESC;
        """)

        result = await self.session.execute(query)
        return [dict(row) for row in result.fetchall()]

class QueryOptimizer:
    """æŸ¥è©¢å„ªåŒ–å™¨"""

    @staticmethod
    def optimize_user_queries():
        """å„ªåŒ–ä½¿ç”¨è€…ç›¸é—œæŸ¥è©¢"""
        # ä½¿ç”¨ç´¢å¼•çš„é«˜æ•ˆæŸ¥è©¢
        efficient_queries = {
            "find_active_users": """
                SELECT u.* FROM users u
                WHERE u.is_active = true
                AND u.is_deleted = false
                ORDER BY u.created_at DESC
                LIMIT :limit
            """,

            "find_users_by_email": """
                SELECT u.* FROM users u
                WHERE u.email = :email
                AND u.is_deleted = false
            """,

            "get_user_login_history": """
                SELECT lh.* FROM login_history lh
                WHERE lh.user_id = :user_id
                AND lh.created_at >= :since
                ORDER BY lh.created_at DESC
                LIMIT :limit
            """,

            "count_active_sessions": """
                SELECT COUNT(*) FROM user_sessions us
                WHERE us.user_id = :user_id
                AND us.expires_at > NOW()
                AND us.is_deleted = false
            """
        }

        return efficient_queries

    @staticmethod
    def create_optimized_indexes():
        """å»ºç«‹å„ªåŒ–ç´¢å¼•çš„ SQL"""
        return [
            # è¤‡åˆç´¢å¼•ç”¨æ–¼å¸¸è¦‹æŸ¥è©¢
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_active_email ON users (is_active, email) WHERE is_deleted = false",

            # éƒ¨åˆ†ç´¢å¼•ç”¨æ–¼æ´»èºä½¿ç”¨è€…
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_users_active_created ON users (created_at) WHERE is_active = true AND is_deleted = false",

            # æœƒè©±éæœŸæŸ¥è©¢å„ªåŒ–
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_sessions_user_expires ON user_sessions (user_id, expires_at) WHERE is_deleted = false",

            # ç™»å…¥æ­·å²æ™‚é–“ç¯„åœæŸ¥è©¢
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_login_history_user_time ON login_history (user_id, created_at DESC)",

            # å¤±æ•—ç™»å…¥æŸ¥è©¢
            "CREATE INDEX CONCURRENTLY IF NOT EXISTS idx_login_history_failed ON login_history (ip_address, created_at) WHERE success = false",
        ]

class ConnectionPoolOptimizer:
    """é€£æ¥æ± å„ªåŒ–å™¨"""

    def __init__(self):
        self.metrics = {
            "pool_size": [],
            "overflow": [],
            "checked_out": [],
            "response_times": []
        }

    async def analyze_pool_performance(self, db_manager: DatabaseManager) -> dict:
        """åˆ†æé€£æ¥æ± æ•ˆèƒ½"""
        stats = await db_manager.get_connection_stats()

        # è¨˜éŒ„æŒ‡æ¨™
        self.metrics["pool_size"].append(stats["pool_size"])
        self.metrics["overflow"].append(stats["overflow"])
        self.metrics["checked_out"].append(stats["checked_out"])

        # è¨ˆç®—å»ºè­°
        recommendations = []

        # é€£æ¥æ± æº¢å‡ºæª¢æŸ¥
        if stats["overflow"] > 0:
            recommendations.append("Consider increasing pool_size - overflow detected")

        # é€£æ¥æ± åˆ©ç”¨ç‡æª¢æŸ¥
        utilization = stats["checked_out"] / stats["pool_size"] if stats["pool_size"] > 0 else 0
        if utilization > 0.8:
            recommendations.append("High pool utilization - consider increasing pool_size")
        elif utilization < 0.2:
            recommendations.append("Low pool utilization - consider decreasing pool_size")

        return {
            "current_stats": stats,
            "utilization": utilization,
            "recommendations": recommendations
        }

    def get_optimal_pool_config(self, concurrent_users: int, avg_query_time: float) -> dict:
        """è¨ˆç®—æœ€ä½³é€£æ¥æ± é…ç½®"""
        # åŸºæ–¼ç¶“é©—å…¬å¼è¨ˆç®—
        base_pool_size = min(max(concurrent_users // 10, 5), 50)
        max_overflow = base_pool_size // 2

        # æ ¹æ“šæŸ¥è©¢æ™‚é–“èª¿æ•´
        if avg_query_time > 1.0:  # æ…¢æŸ¥è©¢éœ€è¦æ›´å¤§çš„æ± 
            base_pool_size = int(base_pool_size * 1.5)

        return {
            "pool_size": base_pool_size,
            "max_overflow": max_overflow,
            "pool_timeout": 30,
            "pool_recycle": 3600  # 1 å°æ™‚
        }
```

---

## ğŸ’¾ è³‡æ–™å‚™ä»½èˆ‡ç½é›£æ¢å¾©

### è‡ªå‹•åŒ–å‚™ä»½ç­–ç•¥

```python
# scripts/backup/database_backup.py
class DatabaseBackupManager:
    """è³‡æ–™åº«å‚™ä»½ç®¡ç†å™¨"""

    def __init__(self, config: BackupConfig):
        self.config = config
        self.s3_client = boto3.client('s3')
        self.encryption_key = self._load_encryption_key()

    async def create_full_backup(self) -> BackupResult:
        """å»ºç«‹å®Œæ•´å‚™ä»½"""
        backup_id = f"full-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"

        try:
            # 1. å»ºç«‹ä¸€è‡´æ€§å¿«ç…§
            snapshot_info = await self._create_consistent_snapshot()

            # 2. åŒ¯å‡ºè³‡æ–™åº«
            dump_file = await self._export_database(backup_id)

            # 3. åŠ å¯†å‚™ä»½æª”æ¡ˆ
            encrypted_file = await self._encrypt_backup(dump_file)

            # 4. ä¸Šå‚³åˆ°é›²ç«¯å„²å­˜
            s3_key = await self._upload_to_s3(encrypted_file, backup_id)

            # 5. é©—è­‰å‚™ä»½å®Œæ•´æ€§
            await self._verify_backup_integrity(s3_key)

            # 6. æ›´æ–°å‚™ä»½æ¸…å–®
            backup_metadata = {
                "backup_id": backup_id,
                "type": "full",
                "timestamp": datetime.utcnow().isoformat(),
                "size_bytes": os.path.getsize(encrypted_file),
                "s3_key": s3_key,
                "checksum": self._calculate_checksum(encrypted_file),
                "schema_version": await self._get_schema_version()
            }

            await self._update_backup_registry(backup_metadata)

            # 7. æ¸…ç†æœ¬åœ°æª”æ¡ˆ
            os.remove(dump_file)
            os.remove(encrypted_file)

            return BackupResult(success=True, backup_id=backup_id, metadata=backup_metadata)

        except Exception as e:
            await self._cleanup_failed_backup(backup_id)
            raise BackupError(f"Full backup failed: {str(e)}")

    async def create_incremental_backup(self, base_backup_id: str) -> BackupResult:
        """å»ºç«‹å¢é‡å‚™ä»½"""
        backup_id = f"inc-{datetime.utcnow().strftime('%Y%m%d-%H%M%S')}"

        try:
            # 1. å–å¾—ä¸Šæ¬¡å‚™ä»½çš„æ™‚é–“é»
            base_backup = await self._get_backup_metadata(base_backup_id)
            last_backup_time = base_backup["timestamp"]

            # 2. åŒ¯å‡ºå¢é‡è®Šæ›´
            changes_file = await self._export_incremental_changes(backup_id, last_backup_time)

            # 3. WAL æª”æ¡ˆå‚™ä»½ï¼ˆPostgreSQLï¼‰
            wal_files = await self._backup_wal_files(last_backup_time)

            # 4. å»ºç«‹å¢é‡åŒ…
            incremental_package = await self._create_incremental_package(changes_file, wal_files)

            # 5. åŠ å¯†ä¸¦ä¸Šå‚³
            encrypted_package = await self._encrypt_backup(incremental_package)
            s3_key = await self._upload_to_s3(encrypted_package, backup_id)

            # 6. è¨˜éŒ„å¢é‡å‚™ä»½è³‡è¨Š
            backup_metadata = {
                "backup_id": backup_id,
                "type": "incremental",
                "base_backup_id": base_backup_id,
                "timestamp": datetime.utcnow().isoformat(),
                "size_bytes": os.path.getsize(encrypted_package),
                "s3_key": s3_key,
                "changes_count": await self._count_changes(changes_file)
            }

            await self._update_backup_registry(backup_metadata)

            return BackupResult(success=True, backup_id=backup_id, metadata=backup_metadata)

        except Exception as e:
            await self._cleanup_failed_backup(backup_id)
            raise BackupError(f"Incremental backup failed: {str(e)}")

    async def restore_from_backup(self, backup_id: str, target_db: str, point_in_time: datetime = None):
        """å¾å‚™ä»½æ¢å¾©"""
        try:
            backup_metadata = await self._get_backup_metadata(backup_id)

            if backup_metadata["type"] == "full":
                await self._restore_full_backup(backup_metadata, target_db)
            else:
                # å¢é‡æ¢å¾©éœ€è¦æ‰¾åˆ°åŸºç¤å‚™ä»½
                base_backup = await self._find_base_backup(backup_id)
                await self._restore_incremental_backup(base_backup, backup_metadata, target_db)

            # é»å°æ™‚æ¢å¾©
            if point_in_time:
                await self._restore_to_point_in_time(target_db, point_in_time)

            # é©—è­‰æ¢å¾©çµæœ
            await self._verify_restoration(target_db)

            logger.info(f"Database restored successfully from backup {backup_id}")

        except Exception as e:
            raise RestoreError(f"Restoration failed: {str(e)}")

    async def _export_database(self, backup_id: str) -> str:
        """åŒ¯å‡ºè³‡æ–™åº«"""
        dump_file = f"/tmp/backup_{backup_id}.sql"

        # PostgreSQL pg_dump
        cmd = [
            "pg_dump",
            "--host", self.config.db_host,
            "--port", str(self.config.db_port),
            "--username", self.config.db_user,
            "--dbname", self.config.db_name,
            "--format=custom",
            "--compress=9",
            "--verbose",
            "--file", dump_file
        ]

        # è¨­å®šç’°å¢ƒè®Šæ•¸
        env = os.environ.copy()
        env["PGPASSWORD"] = self.config.db_password

        process = await asyncio.create_subprocess_exec(
            *cmd, env=env, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE
        )

        stdout, stderr = await process.communicate()

        if process.returncode != 0:
            raise BackupError(f"pg_dump failed: {stderr.decode()}")

        return dump_file

    async def schedule_automatic_backups(self):
        """æ’ç¨‹è‡ªå‹•å‚™ä»½"""
        scheduler = AsyncIOScheduler()

        # æ¯æ—¥å®Œæ•´å‚™ä»½ (å‡Œæ™¨ 2 é»)
        scheduler.add_job(
            self.create_full_backup,
            trigger="cron",
            hour=2,
            minute=0,
            id="daily_full_backup"
        )

        # æ¯ 4 å°æ™‚å¢é‡å‚™ä»½
        scheduler.add_job(
            self._create_scheduled_incremental_backup,
            trigger="interval",
            hours=4,
            id="incremental_backup"
        )

        # æ¯é€±å®Œæ•´å‚™ä»½é©—è­‰
        scheduler.add_job(
            self._verify_all_backups,
            trigger="cron",
            day_of_week=0,  # æ˜ŸæœŸæ—¥
            hour=1,
            id="weekly_backup_verification"
        )

        scheduler.start()

class BackupRetentionManager:
    """å‚™ä»½ä¿ç•™ç®¡ç†å™¨"""

    def __init__(self, config: RetentionConfig):
        self.config = config

    async def apply_retention_policy(self):
        """æ‡‰ç”¨ä¿ç•™ç­–ç•¥"""
        all_backups = await self._get_all_backups()

        # ä¾æ“šç­–ç•¥åˆ†é¡å‚™ä»½
        retention_groups = {
            "daily": timedelta(days=self.config.daily_retention_days),
            "weekly": timedelta(weeks=self.config.weekly_retention_weeks),
            "monthly": timedelta(days=self.config.monthly_retention_months * 30),
            "yearly": timedelta(days=self.config.yearly_retention_years * 365)
        }

        for group, retention_period in retention_groups.items():
            expired_backups = await self._find_expired_backups(all_backups, retention_period, group)

            for backup in expired_backups:
                await self._delete_backup(backup["backup_id"])
                logger.info(f"Deleted expired backup {backup['backup_id']} from {group} group")

    async def _find_expired_backups(self, all_backups: List[dict], retention_period: timedelta, group: str) -> List[dict]:
        """å°‹æ‰¾éæœŸçš„å‚™ä»½"""
        cutoff_date = datetime.utcnow() - retention_period

        expired = []
        for backup in all_backups:
            backup_date = datetime.fromisoformat(backup["timestamp"])
            if backup_date < cutoff_date and backup.get("retention_group") == group:
                expired.append(backup)

        return expired
```

---

## ğŸ” è³‡æ–™åº«ç›£æ§èˆ‡å‘Šè­¦

### å³æ™‚ç›£æ§ç³»çµ±

```python
# app/monitoring/database_monitor.py
class DatabaseMonitor:
    """è³‡æ–™åº«ç›£æ§ç³»çµ±"""

    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.metrics_collector = PrometheusMetrics()
        self.alert_manager = AlertManager()

    async def start_monitoring(self):
        """é–‹å§‹ç›£æ§"""
        # å•Ÿå‹•å„ç¨®ç›£æ§ä»»å‹™
        await asyncio.gather(
            self._monitor_connection_pool(),
            self._monitor_query_performance(),
            self._monitor_database_size(),
            self._monitor_replication_lag(),
            self._monitor_lock_waits()
        )

    async def _monitor_connection_pool(self):
        """ç›£æ§é€£æ¥æ± """
        while True:
            try:
                stats = await self.db_manager.get_connection_stats()

                # è¨˜éŒ„æŒ‡æ¨™
                self.metrics_collector.gauge("db_pool_size", stats["pool_size"])
                self.metrics_collector.gauge("db_pool_checked_out", stats["checked_out"])
                self.metrics_collector.gauge("db_pool_overflow", stats["overflow"])

                # æª¢æŸ¥å‘Šè­¦æ¢ä»¶
                if stats["overflow"] > 0:
                    await self.alert_manager.send_alert(
                        severity="warning",
                        title="Database Pool Overflow",
                        description=f"Pool overflow detected: {stats['overflow']} connections"
                    )

                utilization = stats["checked_out"] / stats["pool_size"] if stats["pool_size"] > 0 else 0
                if utilization > 0.9:
                    await self.alert_manager.send_alert(
                        severity="critical",
                        title="High Database Pool Utilization",
                        description=f"Pool utilization: {utilization:.2%}"
                    )

                await asyncio.sleep(30)

            except Exception as e:
                logger.error("Connection pool monitoring failed", error=str(e))
                await asyncio.sleep(60)

    async def _monitor_query_performance(self):
        """ç›£æ§æŸ¥è©¢æ•ˆèƒ½"""
        while True:
            try:
                async with self.db_manager.get_session() as session:
                    # å–å¾—æ…¢æŸ¥è©¢çµ±è¨ˆ
                    slow_queries = await self._get_slow_queries(session)

                    for query in slow_queries:
                        self.metrics_collector.histogram(
                            "db_query_duration",
                            query["mean_time"],
                            labels={"query_type": query["query_type"]}
                        )

                        # æ…¢æŸ¥è©¢å‘Šè­¦
                        if query["mean_time"] > 5000:  # 5 ç§’
                            await self.alert_manager.send_alert(
                                severity="warning",
                                title="Slow Query Detected",
                                description=f"Query taking {query['mean_time']}ms on average"
                            )

                    # ç›£æ§é–ç­‰å¾…
                    lock_waits = await self._get_lock_waits(session)
                    self.metrics_collector.gauge("db_lock_waits", len(lock_waits))

                    if len(lock_waits) > 10:
                        await self.alert_manager.send_alert(
                            severity="critical",
                            title="High Lock Contention",
                            description=f"{len(lock_waits)} queries waiting for locks"
                        )

                await asyncio.sleep(60)

            except Exception as e:
                logger.error("Query performance monitoring failed", error=str(e))
                await asyncio.sleep(120)

    async def _get_slow_queries(self, session: AsyncSession) -> List[dict]:
        """å–å¾—æ…¢æŸ¥è©¢çµ±è¨ˆ"""
        # PostgreSQL pg_stat_statements
        query = text("""
        SELECT
            query,
            calls,
            total_time,
            mean_time,
            min_time,
            max_time,
            stddev_time,
            rows
        FROM pg_stat_statements
        WHERE mean_time > 100  -- è¶…é 100ms
        ORDER BY mean_time DESC
        LIMIT 20;
        """)

        result = await session.execute(query)
        return [dict(row) for row in result.fetchall()]

    async def _monitor_database_size(self):
        """ç›£æ§è³‡æ–™åº«å¤§å°"""
        while True:
            try:
                async with self.db_manager.get_session() as session:
                    # è³‡æ–™åº«å¤§å°
                    db_size = await self._get_database_size(session)
                    self.metrics_collector.gauge("db_size_bytes", db_size["size_bytes"])

                    # è¡¨æ ¼å¤§å°
                    table_sizes = await self._get_table_sizes(session)
                    for table in table_sizes:
                        self.metrics_collector.gauge(
                            "db_table_size_bytes",
                            table["size_bytes"],
                            labels={"table": table["table_name"]}
                        )

                    # æª¢æŸ¥ç£ç¢Ÿç©ºé–“å‘Šè­¦
                    if db_size["size_bytes"] > 50 * 1024**3:  # 50GB
                        await self.alert_manager.send_alert(
                            severity="warning",
                            title="Database Size Alert",
                            description=f"Database size: {db_size['size_gb']:.1f}GB"
                        )

                await asyncio.sleep(3600)  # æ¯å°æ™‚æª¢æŸ¥

            except Exception as e:
                logger.error("Database size monitoring failed", error=str(e))
                await asyncio.sleep(3600)

    async def generate_health_report(self) -> dict:
        """ç”Ÿæˆå¥åº·å ±å‘Š"""
        async with self.db_manager.get_session() as session:
            report = {
                "timestamp": datetime.utcnow().isoformat(),
                "connection_pool": await self.db_manager.get_connection_stats(),
                "database_size": await self._get_database_size(session),
                "slow_queries": await self._get_slow_queries(session),
                "lock_waits": await self._get_lock_waits(session),
                "index_usage": await self._get_index_usage_stats(session),
                "replication_status": await self._get_replication_status(session)
            }

        return report

class DatabaseAlerting:
    """è³‡æ–™åº«å‘Šè­¦ç³»çµ±"""

    def __init__(self):
        self.alert_rules = self._load_alert_rules()
        self.notification_channels = self._setup_notification_channels()

    def _load_alert_rules(self) -> List[AlertRule]:
        """è¼‰å…¥å‘Šè­¦è¦å‰‡"""
        return [
            AlertRule(
                name="high_connection_usage",
                condition="db_pool_utilization > 0.8",
                severity="warning",
                duration="5m"
            ),
            AlertRule(
                name="query_timeout",
                condition="db_query_duration > 30s",
                severity="critical",
                duration="1m"
            ),
            AlertRule(
                name="replication_lag",
                condition="db_replication_lag > 60s",
                severity="warning",
                duration="2m"
            ),
            AlertRule(
                name="deadlock_detected",
                condition="rate(db_deadlocks_total[5m]) > 0",
                severity="critical",
                duration="0s"
            )
        ]

    async def evaluate_rules(self, metrics: dict):
        """è©•ä¼°å‘Šè­¦è¦å‰‡"""
        for rule in self.alert_rules:
            if await self._evaluate_condition(rule.condition, metrics):
                await self._fire_alert(rule, metrics)

    async def _fire_alert(self, rule: AlertRule, metrics: dict):
        """è§¸ç™¼å‘Šè­¦"""
        alert = Alert(
            rule_name=rule.name,
            severity=rule.severity,
            message=self._format_alert_message(rule, metrics),
            timestamp=datetime.utcnow(),
            metrics=metrics
        )

        # ç™¼é€åˆ°å„å€‹é€šçŸ¥é€šé“
        for channel in self.notification_channels:
            await channel.send_alert(alert)
```

---

## ğŸ’¡ å­¸ç¿’ç­†è¨˜å€

### ğŸ¤” æˆ‘çš„ç†è§£
```
SQLAlchemy çš„é€²éšç‰¹æ€§ï¼š

è³‡æ–™åº«é·ç§»çš„æœ€ä½³å¯¦å‹™ï¼š

æŸ¥è©¢å„ªåŒ–çš„é—œéµç­–ç•¥ï¼š

è³‡æ–™åº«ç›£æ§çš„é‡è¦æŒ‡æ¨™ï¼š
```

### ğŸ’¾ å¯¦è¸å¿ƒå¾—
```
è¨­è¨ˆè³‡æ–™åº«æ¶æ§‹çš„è€ƒé‡ï¼š

æ•ˆèƒ½å„ªåŒ–çš„å¯¦æˆ°ç¶“é©—ï¼š

å‚™ä»½ç­–ç•¥çš„é‡è¦æ€§ï¼š

ç”Ÿç”¢ç’°å¢ƒçš„é‹ç¶­æŒ‘æˆ°ï¼š
```

### ğŸš€ é€²éšæ€è€ƒ
```
åˆ†æ•£å¼è³‡æ–™åº«çš„æŒ‘æˆ°ï¼š

é›²åŸç”Ÿè³‡æ–™åº«çš„å„ªå‹¢ï¼š

è³‡æ–™åº«å®‰å…¨çš„è€ƒé‡ï¼š

æœªä¾†è³‡æ–™åº«æŠ€è¡“è¶¨å‹¢ï¼š
```