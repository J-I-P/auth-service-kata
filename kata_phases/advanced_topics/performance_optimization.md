# Performance Optimization: ä¼æ¥­ç´šæ•ˆèƒ½èª¿å„ªèˆ‡æ“´å±•ç­–ç•¥

## ğŸ¯ å­¸ç¿’ç›®æ¨™
- æŒæ¡ç¾ä»£ Web æ‡‰ç”¨æ•ˆèƒ½åˆ†æèˆ‡å„ªåŒ–æ–¹æ³•è«–
- å­¸ç¿’ FastAPI èˆ‡ Python æ•ˆèƒ½èª¿å„ªæœ€ä½³å¯¦å‹™
- å»ºç«‹å¯æ“´å±•çš„å¿«å–èˆ‡è² è¼‰å¹³è¡¡ç­–ç•¥
- å¯¦è¸æ•ˆèƒ½ç›£æ§èˆ‡è‡ªå‹•åŒ–å„ªåŒ–ç³»çµ±

---

## ğŸ” æ•ˆèƒ½åˆ†æèˆ‡ç“¶é ¸è­˜åˆ¥

### å…¨é¢æ•ˆèƒ½åˆ†ææ¡†æ¶

```python
# app/performance/profiler.py
import cProfile
import pstats
import time
import asyncio
from functools import wraps
from typing import Dict, List, Callable, Any
from dataclasses import dataclass
import structlog

logger = structlog.get_logger()

@dataclass
class PerformanceMetrics:
    """æ•ˆèƒ½æŒ‡æ¨™è³‡æ–™çµæ§‹"""
    function_name: str
    execution_time: float
    memory_usage: int
    cpu_usage: float
    call_count: int
    avg_time_per_call: float

class PerformanceProfiler:
    """ä¼æ¥­ç´šæ•ˆèƒ½åˆ†æå™¨"""

    def __init__(self):
        self.metrics = {}
        self.active_sessions = {}

    def profile_function(self, track_memory: bool = True, track_cpu: bool = True):
        """å‡½æ•¸æ•ˆèƒ½åˆ†æè£é£¾å™¨"""
        def decorator(func: Callable):
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                start_time = time.perf_counter()
                start_memory = self._get_memory_usage() if track_memory else 0
                start_cpu = self._get_cpu_usage() if track_cpu else 0

                try:
                    # åŸ·è¡Œå‡½æ•¸
                    if asyncio.iscoroutinefunction(func):
                        result = await func(*args, **kwargs)
                    else:
                        result = func(*args, **kwargs)

                    return result

                finally:
                    # è¨ˆç®—æ•ˆèƒ½æŒ‡æ¨™
                    end_time = time.perf_counter()
                    execution_time = end_time - start_time

                    end_memory = self._get_memory_usage() if track_memory else 0
                    end_cpu = self._get_cpu_usage() if track_cpu else 0

                    # è¨˜éŒ„æŒ‡æ¨™
                    await self._record_metrics(
                        function_name=func.__name__,
                        execution_time=execution_time,
                        memory_delta=end_memory - start_memory,
                        cpu_usage=end_cpu - start_cpu
                    )

            return async_wrapper
        return decorator

    async def _record_metrics(self, function_name: str, execution_time: float,
                            memory_delta: int, cpu_usage: float):
        """è¨˜éŒ„æ•ˆèƒ½æŒ‡æ¨™"""
        if function_name not in self.metrics:
            self.metrics[function_name] = {
                "total_time": 0,
                "call_count": 0,
                "min_time": float('inf'),
                "max_time": 0,
                "memory_peak": 0,
                "cpu_peak": 0
            }

        metrics = self.metrics[function_name]
        metrics["total_time"] += execution_time
        metrics["call_count"] += 1
        metrics["min_time"] = min(metrics["min_time"], execution_time)
        metrics["max_time"] = max(metrics["max_time"], execution_time)
        metrics["memory_peak"] = max(metrics["memory_peak"], memory_delta)
        metrics["cpu_peak"] = max(metrics["cpu_peak"], cpu_usage)

        # è¨˜éŒ„æ…¢å‡½æ•¸
        if execution_time > 1.0:  # è¶…é 1 ç§’
            logger.warning("Slow function execution",
                         function=function_name,
                         duration=execution_time,
                         memory_delta=memory_delta)

    def get_performance_report(self) -> List[PerformanceMetrics]:
        """ç”Ÿæˆæ•ˆèƒ½å ±å‘Š"""
        report = []
        for func_name, data in self.metrics.items():
            avg_time = data["total_time"] / data["call_count"] if data["call_count"] > 0 else 0

            metrics = PerformanceMetrics(
                function_name=func_name,
                execution_time=data["total_time"],
                memory_usage=data["memory_peak"],
                cpu_usage=data["cpu_peak"],
                call_count=data["call_count"],
                avg_time_per_call=avg_time
            )
            report.append(metrics)

        # æŒ‰å¹³å‡åŸ·è¡Œæ™‚é–“æ’åº
        return sorted(report, key=lambda x: x.avg_time_per_call, reverse=True)

    def _get_memory_usage(self) -> int:
        """å–å¾—è¨˜æ†¶é«”ä½¿ç”¨é‡"""
        import psutil
        process = psutil.Process()
        return process.memory_info().rss

    def _get_cpu_usage(self) -> float:
        """å–å¾— CPU ä½¿ç”¨ç‡"""
        import psutil
        return psutil.cpu_percent()

class AsyncProfiler:
    """ç•°æ­¥ç¨‹å¼æ•ˆèƒ½åˆ†æå™¨"""

    def __init__(self):
        self.async_metrics = {}
        self.concurrency_stats = {}

    async def profile_async_endpoint(self, endpoint_name: str):
        """åˆ†æç•°æ­¥ endpoint æ•ˆèƒ½"""
        async def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                request_id = id(asyncio.current_task())
                start_time = time.perf_counter()

                # è¨˜éŒ„ä½µç™¼è«‹æ±‚æ•¸
                self._increment_concurrency(endpoint_name)

                try:
                    result = await func(*args, **kwargs)
                    return result

                finally:
                    end_time = time.perf_counter()
                    execution_time = end_time - start_time

                    await self._record_async_metrics(endpoint_name, execution_time, request_id)
                    self._decrement_concurrency(endpoint_name)

            return wrapper
        return decorator

    def _increment_concurrency(self, endpoint_name: str):
        """å¢åŠ ä½µç™¼è¨ˆæ•¸"""
        if endpoint_name not in self.concurrency_stats:
            self.concurrency_stats[endpoint_name] = {"current": 0, "peak": 0}

        self.concurrency_stats[endpoint_name]["current"] += 1
        current = self.concurrency_stats[endpoint_name]["current"]
        self.concurrency_stats[endpoint_name]["peak"] = max(
            self.concurrency_stats[endpoint_name]["peak"], current
        )

    def _decrement_concurrency(self, endpoint_name: str):
        """æ¸›å°‘ä½µç™¼è¨ˆæ•¸"""
        if endpoint_name in self.concurrency_stats:
            self.concurrency_stats[endpoint_name]["current"] -= 1

    async def analyze_async_performance(self) -> Dict[str, Any]:
        """åˆ†æç•°æ­¥æ•ˆèƒ½"""
        analysis = {}

        for endpoint, stats in self.concurrency_stats.items():
            analysis[endpoint] = {
                "peak_concurrency": stats["peak"],
                "current_concurrency": stats["current"],
                "bottleneck_risk": "high" if stats["peak"] > 50 else "low"
            }

        return analysis

class MemoryProfiler:
    """è¨˜æ†¶é«”ä½¿ç”¨åˆ†æå™¨"""

    def __init__(self):
        self.memory_snapshots = []

    async def profile_memory_usage(self, interval: float = 1.0, duration: float = 60.0):
        """ç›£æ§è¨˜æ†¶é«”ä½¿ç”¨æƒ…æ³"""
        import psutil
        import gc

        start_time = time.time()
        process = psutil.Process()

        while time.time() - start_time < duration:
            snapshot = {
                "timestamp": time.time(),
                "rss": process.memory_info().rss,
                "vms": process.memory_info().vms,
                "percent": process.memory_percent(),
                "gc_stats": {
                    "generation_0": gc.get_count()[0],
                    "generation_1": gc.get_count()[1],
                    "generation_2": gc.get_count()[2],
                }
            }

            self.memory_snapshots.append(snapshot)
            await asyncio.sleep(interval)

    def detect_memory_leaks(self) -> List[Dict[str, Any]]:
        """æª¢æ¸¬è¨˜æ†¶é«”æ´©æ¼"""
        if len(self.memory_snapshots) < 10:
            return []

        leaks = []
        window_size = 10

        for i in range(window_size, len(self.memory_snapshots)):
            recent_avg = sum(s["rss"] for s in self.memory_snapshots[i-window_size:i]) / window_size
            current = self.memory_snapshots[i]["rss"]

            # è¨˜æ†¶é«”å¢é•·è¶…é 50%
            if current > recent_avg * 1.5:
                leaks.append({
                    "timestamp": self.memory_snapshots[i]["timestamp"],
                    "memory_usage": current,
                    "growth_rate": (current - recent_avg) / recent_avg,
                    "severity": "high" if current > recent_avg * 2 else "medium"
                })

        return leaks

    def get_memory_recommendations(self) -> List[str]:
        """å–å¾—è¨˜æ†¶é«”å„ªåŒ–å»ºè­°"""
        recommendations = []

        if not self.memory_snapshots:
            return recommendations

        latest = self.memory_snapshots[-1]
        peak_memory = max(s["rss"] for s in self.memory_snapshots)

        # è¨˜æ†¶é«”ä½¿ç”¨ç‡é«˜
        if latest["percent"] > 80:
            recommendations.append("Consider increasing available memory or optimizing memory usage")

        # åƒåœ¾å›æ”¶é »ç‡é«˜
        if latest["gc_stats"]["generation_0"] > 1000:
            recommendations.append("High garbage collection activity - consider object pooling")

        # è¨˜æ†¶é«”å³°å€¼éé«˜
        if peak_memory > 2 * 1024**3:  # 2GB
            recommendations.append("Peak memory usage is high - analyze large object allocations")

        return recommendations
```

---

## âš¡ FastAPI æ•ˆèƒ½å„ªåŒ–

### é«˜æ•ˆèƒ½ FastAPI é…ç½®

```python
# app/performance/fastapi_optimization.py
from fastapi import FastAPI, Request, Response
from fastapi.middleware.gzip import GZipMiddleware
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from uvloop import EventLoopPolicy
import asyncio

class OptimizedFastAPI:
    """å„ªåŒ–çš„ FastAPI æ‡‰ç”¨"""

    @staticmethod
    def create_optimized_app() -> FastAPI:
        """å»ºç«‹å„ªåŒ–çš„ FastAPI æ‡‰ç”¨"""
        app = FastAPI(
            title="Auth Service",
            description="High-performance authentication service",
            version="1.0.0",
            # æ•ˆèƒ½å„ªåŒ–è¨­å®š
            docs_url="/docs" if settings.ENVIRONMENT != "production" else None,
            redoc_url="/redoc" if settings.ENVIRONMENT != "production" else None,
            openapi_url="/openapi.json" if settings.ENVIRONMENT != "production" else None,
            # æ¸›å°‘ OpenAPI ç”Ÿæˆé–‹éŠ·
            generate_unique_id_function=lambda route: f"{route.tags[0]}-{route.name}" if route.tags else route.name,
        )

        # æ•ˆèƒ½ä¸­é–“ä»¶
        OptimizedFastAPI._setup_performance_middleware(app)

        # ç•°æ­¥äº‹ä»¶å¾ªç’°å„ªåŒ–
        OptimizedFastAPI._setup_event_loop_optimization()

        return app

    @staticmethod
    def _setup_performance_middleware(app: FastAPI):
        """è¨­å®šæ•ˆèƒ½ä¸­é–“ä»¶"""

        # 1. éŸ¿æ‡‰å£“ç¸®
        app.add_middleware(GZipMiddleware, minimum_size=1000)

        # 2. éŸ¿æ‡‰å¿«å–ä¸­é–“ä»¶
        @app.middleware("http")
        async def cache_middleware(request: Request, call_next):
            # åªå¿«å– GET è«‹æ±‚
            if request.method == "GET":
                cache_key = f"response:{request.url.path}:{hash(str(request.query_params))}"
                cached_response = await cache.get(cache_key)

                if cached_response:
                    return Response(
                        content=cached_response["content"],
                        status_code=cached_response["status_code"],
                        headers=cached_response["headers"],
                        media_type=cached_response["media_type"]
                    )

            response = await call_next(request)

            # å¿«å–æˆåŠŸéŸ¿æ‡‰
            if request.method == "GET" and response.status_code == 200:
                # è®€å–éŸ¿æ‡‰å…§å®¹
                response_body = b""
                async for chunk in response.body_iterator:
                    response_body += chunk

                # å­˜å…¥å¿«å–
                await cache.set(cache_key, {
                    "content": response_body,
                    "status_code": response.status_code,
                    "headers": dict(response.headers),
                    "media_type": response.media_type
                }, expire=300)  # 5 åˆ†é˜å¿«å–

                # é‡å»ºéŸ¿æ‡‰
                return Response(
                    content=response_body,
                    status_code=response.status_code,
                    headers=response.headers,
                    media_type=response.media_type
                )

            return response

        # 3. æ•ˆèƒ½ç›£æ§ä¸­é–“ä»¶
        @app.middleware("http")
        async def performance_middleware(request: Request, call_next):
            start_time = time.perf_counter()

            response = await call_next(request)

            process_time = time.perf_counter() - start_time
            response.headers["X-Process-Time"] = str(process_time)

            # è¨˜éŒ„æ…¢è«‹æ±‚
            if process_time > 1.0:
                logger.warning("Slow request detected",
                             path=request.url.path,
                             method=request.method,
                             duration=process_time)

            return response

    @staticmethod
    def _setup_event_loop_optimization():
        """è¨­å®šäº‹ä»¶å¾ªç’°å„ªåŒ–"""
        # ä½¿ç”¨ uvloop æå‡æ•ˆèƒ½ (Unix only)
        import platform
        if platform.system() != 'Windows':
            asyncio.set_event_loop_policy(EventLoopPolicy())

class ConnectionPoolOptimizer:
    """é€£æ¥æ± å„ªåŒ–å™¨"""

    @staticmethod
    def create_optimized_http_client():
        """å»ºç«‹å„ªåŒ–çš„ HTTP å®¢æˆ¶ç«¯"""
        import httpx

        # é€£æ¥æ± é…ç½®
        limits = httpx.Limits(
            max_keepalive_connections=100,
            max_connections=200,
            keepalive_expiry=30
        )

        # è¶…æ™‚é…ç½®
        timeout = httpx.Timeout(
            connect=5.0,
            read=30.0,
            write=10.0,
            pool=5.0
        )

        return httpx.AsyncClient(
            limits=limits,
            timeout=timeout,
            http2=True,  # å•Ÿç”¨ HTTP/2
            verify=True
        )

    @staticmethod
    def optimize_database_connections():
        """å„ªåŒ–è³‡æ–™åº«é€£æ¥"""
        return {
            "pool_size": 20,
            "max_overflow": 30,
            "pool_timeout": 30,
            "pool_recycle": 3600,
            "pool_pre_ping": True,
            "echo": False,
            "echo_pool": False
        }

class RequestOptimizer:
    """è«‹æ±‚è™•ç†å„ªåŒ–å™¨"""

    @staticmethod
    async def optimize_pydantic_parsing():
        """å„ªåŒ– Pydantic è§£ææ•ˆèƒ½"""
        from pydantic import BaseModel
        from pydantic.json import pydantic_encoder

        # ä½¿ç”¨ orjson åŠ é€Ÿ JSON åºåˆ—åŒ–
        import orjson

        def custom_json_encoder(obj):
            return orjson.dumps(obj, default=pydantic_encoder).decode()

        # ç‚ºæ¨¡å‹è¨­å®šè‡ªå®šç¾©ç·¨ç¢¼å™¨
        BaseModel.__json_encoder__ = custom_json_encoder

    @staticmethod
    def create_response_model_optimization():
        """éŸ¿æ‡‰æ¨¡å‹å„ªåŒ–"""
        from pydantic import BaseModel, Field
        from typing import Optional, List

        class OptimizedBaseModel(BaseModel):
            """å„ªåŒ–çš„åŸºç¤æ¨¡å‹"""

            class Config:
                # æ•ˆèƒ½å„ªåŒ–è¨­å®š
                allow_reuse=True
                validate_assignment=False  # æ¸›å°‘é©—è­‰é–‹éŠ·
                use_enum_values=True
                json_encoders={
                    datetime: lambda dt: dt.isoformat(),
                    UUID: str
                }

                # åºåˆ—åŒ–å„ªåŒ–
                anystr_strip_whitespace=True
                min_anystr_length=0

        return OptimizedBaseModel

class BackgroundTaskOptimizer:
    """èƒŒæ™¯ä»»å‹™å„ªåŒ–å™¨"""

    def __init__(self):
        self.task_queue = asyncio.Queue(maxsize=1000)
        self.workers = []
        self.metrics = {
            "tasks_completed": 0,
            "tasks_failed": 0,
            "avg_processing_time": 0
        }

    async def start_workers(self, num_workers: int = 4):
        """å•Ÿå‹•èƒŒæ™¯å·¥ä½œè€…"""
        for i in range(num_workers):
            worker = asyncio.create_task(self._worker(f"worker-{i}"))
            self.workers.append(worker)

    async def _worker(self, name: str):
        """èƒŒæ™¯å·¥ä½œè€…"""
        while True:
            try:
                # å¾ä½‡åˆ—å–å¾—ä»»å‹™
                task_func, args, kwargs = await self.task_queue.get()

                start_time = time.perf_counter()

                # åŸ·è¡Œä»»å‹™
                try:
                    if asyncio.iscoroutinefunction(task_func):
                        await task_func(*args, **kwargs)
                    else:
                        task_func(*args, **kwargs)

                    self.metrics["tasks_completed"] += 1

                except Exception as e:
                    logger.error(f"Background task failed in {name}", error=str(e))
                    self.metrics["tasks_failed"] += 1

                finally:
                    processing_time = time.perf_counter() - start_time
                    self._update_avg_processing_time(processing_time)

                    # æ¨™è¨˜ä»»å‹™å®Œæˆ
                    self.task_queue.task_done()

            except Exception as e:
                logger.error(f"Worker {name} error", error=str(e))
                await asyncio.sleep(1)

    async def submit_task(self, func, *args, **kwargs):
        """æäº¤èƒŒæ™¯ä»»å‹™"""
        try:
            await self.task_queue.put((func, args, kwargs))
        except asyncio.QueueFull:
            logger.warning("Background task queue is full, dropping task")

    def _update_avg_processing_time(self, new_time: float):
        """æ›´æ–°å¹³å‡è™•ç†æ™‚é–“"""
        total_tasks = self.metrics["tasks_completed"] + self.metrics["tasks_failed"]
        if total_tasks > 0:
            current_avg = self.metrics["avg_processing_time"]
            self.metrics["avg_processing_time"] = (current_avg * (total_tasks - 1) + new_time) / total_tasks

    async def get_queue_stats(self) -> dict:
        """å–å¾—ä½‡åˆ—çµ±è¨ˆ"""
        return {
            "queue_size": self.task_queue.qsize(),
            "workers_active": len(self.workers),
            "metrics": self.metrics.copy()
        }
```

---

## ğŸš€ å¿«å–ç­–ç•¥èˆ‡å„ªåŒ–

### å¤šå±¤æ¬¡å¿«å–æ¶æ§‹

```python
# app/cache/cache_manager.py
from abc import ABC, abstractmethod
from typing import Any, Optional, Union, List, Dict
import asyncio
import json
import time
from dataclasses import dataclass

@dataclass
class CacheStats:
    """å¿«å–çµ±è¨ˆ"""
    hits: int = 0
    misses: int = 0
    sets: int = 0
    deletes: int = 0
    evictions: int = 0

    @property
    def hit_rate(self) -> float:
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0

class CacheBackend(ABC):
    """å¿«å–å¾Œç«¯æŠ½è±¡åŸºé¡"""

    @abstractmethod
    async def get(self, key: str) -> Optional[Any]:
        pass

    @abstractmethod
    async def set(self, key: str, value: Any, expire: int = None) -> bool:
        pass

    @abstractmethod
    async def delete(self, key: str) -> bool:
        pass

    @abstractmethod
    async def exists(self, key: str) -> bool:
        pass

class InMemoryCache(CacheBackend):
    """è¨˜æ†¶é«”å¿«å–å¯¦ä½œ"""

    def __init__(self, max_size: int = 10000, default_ttl: int = 3600):
        self.max_size = max_size
        self.default_ttl = default_ttl
        self.data = {}
        self.expiry = {}
        self.access_times = {}
        self.stats = CacheStats()

    async def get(self, key: str) -> Optional[Any]:
        # æª¢æŸ¥éæœŸ
        if key in self.expiry and time.time() > self.expiry[key]:
            await self.delete(key)
            return None

        if key in self.data:
            self.access_times[key] = time.time()
            self.stats.hits += 1
            return self.data[key]

        self.stats.misses += 1
        return None

    async def set(self, key: str, value: Any, expire: int = None) -> bool:
        # æª¢æŸ¥å®¹é‡é™åˆ¶
        if len(self.data) >= self.max_size and key not in self.data:
            await self._evict_lru()

        self.data[key] = value
        self.access_times[key] = time.time()

        # è¨­å®šéæœŸæ™‚é–“
        if expire:
            self.expiry[key] = time.time() + expire
        else:
            self.expiry[key] = time.time() + self.default_ttl

        self.stats.sets += 1
        return True

    async def delete(self, key: str) -> bool:
        if key in self.data:
            del self.data[key]
            self.expiry.pop(key, None)
            self.access_times.pop(key, None)
            self.stats.deletes += 1
            return True
        return False

    async def exists(self, key: str) -> bool:
        return key in self.data and (key not in self.expiry or time.time() <= self.expiry[key])

    async def _evict_lru(self):
        """ç§»é™¤æœ€ä¹…æœªä½¿ç”¨çš„é …ç›®"""
        if not self.access_times:
            return

        lru_key = min(self.access_times, key=self.access_times.get)
        await self.delete(lru_key)
        self.stats.evictions += 1

class RedisCache(CacheBackend):
    """Redis å¿«å–å¯¦ä½œ"""

    def __init__(self, redis_client, default_ttl: int = 3600):
        self.redis = redis_client
        self.default_ttl = default_ttl
        self.stats = CacheStats()

    async def get(self, key: str) -> Optional[Any]:
        try:
            result = await self.redis.get(key)
            if result is not None:
                self.stats.hits += 1
                return json.loads(result)
            else:
                self.stats.misses += 1
                return None
        except Exception as e:
            logger.error("Redis get error", key=key, error=str(e))
            return None

    async def set(self, key: str, value: Any, expire: int = None) -> bool:
        try:
            ttl = expire or self.default_ttl
            serialized = json.dumps(value, default=str)
            await self.redis.setex(key, ttl, serialized)
            self.stats.sets += 1
            return True
        except Exception as e:
            logger.error("Redis set error", key=key, error=str(e))
            return False

    async def delete(self, key: str) -> bool:
        try:
            result = await self.redis.delete(key)
            if result:
                self.stats.deletes += 1
            return bool(result)
        except Exception as e:
            logger.error("Redis delete error", key=key, error=str(e))
            return False

    async def exists(self, key: str) -> bool:
        try:
            return bool(await self.redis.exists(key))
        except Exception as e:
            logger.error("Redis exists error", key=key, error=str(e))
            return False

class MultiLayerCache:
    """å¤šå±¤æ¬¡å¿«å–ç®¡ç†å™¨"""

    def __init__(self, layers: List[CacheBackend]):
        self.layers = layers  # æŒ‰å„ªå…ˆç´šæ’åºï¼Œç¬¬ä¸€å±¤æ˜¯æœ€å¿«çš„

    async def get(self, key: str) -> Optional[Any]:
        """å¾å¿«å–å±¤ä¸­å–å¾—å€¼"""
        for i, layer in enumerate(self.layers):
            value = await layer.get(key)
            if value is not None:
                # å°‡å€¼å¯«å…¥è¼ƒé«˜å„ªå…ˆç´šçš„å±¤
                await self._write_to_upper_layers(key, value, i)
                return value
        return None

    async def set(self, key: str, value: Any, expire: int = None) -> bool:
        """è¨­å®šå€¼åˆ°æ‰€æœ‰å¿«å–å±¤"""
        results = []
        for layer in self.layers:
            result = await layer.set(key, value, expire)
            results.append(result)
        return any(results)  # è‡³å°‘ä¸€å±¤æˆåŠŸå³å¯

    async def delete(self, key: str) -> bool:
        """å¾æ‰€æœ‰å¿«å–å±¤åˆªé™¤"""
        results = []
        for layer in self.layers:
            result = await layer.delete(key)
            results.append(result)
        return any(results)

    async def _write_to_upper_layers(self, key: str, value: Any, found_at_layer: int):
        """å°‡å€¼å¯«å…¥æ›´é«˜å„ªå…ˆç´šçš„å±¤"""
        for i in range(found_at_layer):
            await self.layers[i].set(key, value)

    async def get_stats(self) -> Dict[str, CacheStats]:
        """å–å¾—æ‰€æœ‰å±¤çš„çµ±è¨ˆ"""
        stats = {}
        for i, layer in enumerate(self.layers):
            stats[f"layer_{i}"] = layer.stats
        return stats

class SmartCacheManager:
    """æ™ºèƒ½å¿«å–ç®¡ç†å™¨"""

    def __init__(self, cache: MultiLayerCache):
        self.cache = cache
        self.access_patterns = {}
        self.preload_tasks = set()

    async def get_with_pattern_learning(self, key: str) -> Optional[Any]:
        """å¸¶æ¨¡å¼å­¸ç¿’çš„å–å€¼"""
        # è¨˜éŒ„è¨ªå•æ¨¡å¼
        self._record_access_pattern(key)

        value = await self.cache.get(key)

        # é æ¸¬ç›¸é—œéµå€¼ä¸¦é è¼‰å…¥
        if value is not None:
            await self._preload_related_keys(key)

        return value

    def _record_access_pattern(self, key: str):
        """è¨˜éŒ„è¨ªå•æ¨¡å¼"""
        current_time = time.time()

        if key not in self.access_patterns:
            self.access_patterns[key] = {
                "count": 0,
                "last_access": current_time,
                "access_intervals": []
            }

        pattern = self.access_patterns[key]
        if pattern["last_access"]:
            interval = current_time - pattern["last_access"]
            pattern["access_intervals"].append(interval)

            # åªä¿ç•™æœ€è¿‘ 10 æ¬¡é–“éš”
            if len(pattern["access_intervals"]) > 10:
                pattern["access_intervals"].pop(0)

        pattern["count"] += 1
        pattern["last_access"] = current_time

    async def _preload_related_keys(self, key: str):
        """é è¼‰å…¥ç›¸é—œéµå€¼"""
        # åŸºæ–¼è¨ªå•æ¨¡å¼é æ¸¬ç›¸é—œéµå€¼
        related_keys = self._predict_related_keys(key)

        for related_key in related_keys:
            if related_key not in self.preload_tasks:
                task = asyncio.create_task(self._preload_key(related_key))
                self.preload_tasks.add(task)
                task.add_done_callback(self.preload_tasks.discard)

    def _predict_related_keys(self, key: str) -> List[str]:
        """é æ¸¬ç›¸é—œéµå€¼"""
        # ç°¡å–®çš„é—œè¯é æ¸¬é‚è¼¯
        related_keys = []

        # ç”¨æˆ¶ç›¸é—œæ•¸æ“šçš„é—œè¯
        if "user:" in key:
            user_id = key.split(":")[-1]
            related_keys.extend([
                f"user_profile:{user_id}",
                f"user_permissions:{user_id}",
                f"user_sessions:{user_id}"
            ])

        return related_keys

    async def _preload_key(self, key: str):
        """é è¼‰å…¥å–®å€‹éµå€¼"""
        try:
            # å¦‚æœå¿«å–ä¸­æ²’æœ‰ï¼Œå¾è³‡æ–™åº«è¼‰å…¥
            if not await self.cache.cache.layers[0].exists(key):
                # é€™è£¡æ‡‰è©²èª¿ç”¨å¯¦éš›çš„è³‡æ–™è¼‰å…¥é‚è¼¯
                value = await self._load_from_source(key)
                if value:
                    await self.cache.set(key, value)
        except Exception as e:
            logger.error("Preload failed", key=key, error=str(e))

    async def _load_from_source(self, key: str) -> Optional[Any]:
        """å¾è³‡æ–™ä¾†æºè¼‰å…¥ï¼ˆéœ€è¦å¯¦ä½œå…·é«”é‚è¼¯ï¼‰"""
        # é€™è£¡æ‡‰è©²æ ¹æ“š key çš„é¡å‹å¾å°æ‡‰çš„è³‡æ–™ä¾†æºè¼‰å…¥
        pass

    async def optimize_cache_expiry(self):
        """å„ªåŒ–å¿«å–éæœŸæ™‚é–“"""
        for key, pattern in self.access_patterns.items():
            if len(pattern["access_intervals"]) >= 5:
                # è¨ˆç®—å¹³å‡è¨ªå•é–“éš”
                avg_interval = sum(pattern["access_intervals"]) / len(pattern["access_intervals"])

                # è¨­å®šæœ€å„ªéæœŸæ™‚é–“ï¼ˆè¨ªå•é–“éš”çš„ 2-3 å€ï¼‰
                optimal_ttl = int(avg_interval * 2.5)
                optimal_ttl = max(300, min(optimal_ttl, 86400))  # é™åˆ¶åœ¨ 5åˆ†é˜åˆ° 1å¤©ä¹‹é–“

                # å¦‚æœç•¶å‰å­˜åœ¨è©²éµï¼Œæ›´æ–°å…¶éæœŸæ™‚é–“
                if await self.cache.cache.layers[0].exists(key):
                    value = await self.cache.get(key)
                    if value:
                        await self.cache.set(key, value, optimal_ttl)

class CacheDecorator:
    """å¿«å–è£é£¾å™¨"""

    def __init__(self, cache_manager: SmartCacheManager, default_ttl: int = 3600):
        self.cache_manager = cache_manager
        self.default_ttl = default_ttl

    def cached(self, key_func=None, ttl=None, cache_none=False):
        """å¿«å–è£é£¾å™¨"""
        def decorator(func):
            @wraps(func)
            async def wrapper(*args, **kwargs):
                # ç”Ÿæˆå¿«å–éµ
                if key_func:
                    cache_key = key_func(*args, **kwargs)
                else:
                    cache_key = f"{func.__name__}:{hash(str(args) + str(sorted(kwargs.items())))}"

                # å˜—è©¦å¾å¿«å–å–å¾—
                cached_value = await self.cache_manager.get_with_pattern_learning(cache_key)
                if cached_value is not None:
                    return cached_value

                # åŸ·è¡Œå‡½æ•¸
                result = await func(*args, **kwargs) if asyncio.iscoroutinefunction(func) else func(*args, **kwargs)

                # å¿«å–çµæœ
                if result is not None or cache_none:
                    expire_time = ttl or self.default_ttl
                    await self.cache_manager.cache.set(cache_key, result, expire_time)

                return result

            return wrapper
        return decorator

    def invalidate_pattern(self, pattern: str):
        """ä½¿ç¬¦åˆæ¨¡å¼çš„å¿«å–å¤±æ•ˆ"""
        async def invalidate():
            # é€™éœ€è¦æ”¯æ´æ¨¡å¼åŒ¹é…çš„å¿«å–å¾Œç«¯
            # Redis: SCAN + pattern matching
            # æˆ–ç¶­è­·ä¸€å€‹éµå€¼ç´¢å¼•
            pass

        return invalidate()
```

---

## ğŸ—ï¸ è² è¼‰å¹³è¡¡èˆ‡æ“´å±•

### æ™ºèƒ½è² è¼‰å¹³è¡¡ç­–ç•¥

```python
# app/load_balancing/load_balancer.py
from abc import ABC, abstractmethod
from typing import List, Optional, Dict, Any
from dataclasses import dataclass
import asyncio
import time
import random
from enum import Enum

class BalancingStrategy(Enum):
    ROUND_ROBIN = "round_robin"
    WEIGHTED_ROUND_ROBIN = "weighted_round_robin"
    LEAST_CONNECTIONS = "least_connections"
    LEAST_RESPONSE_TIME = "least_response_time"
    CONSISTENT_HASH = "consistent_hash"

@dataclass
class ServerNode:
    """æœå‹™ç¯€é»"""
    id: str
    host: str
    port: int
    weight: int = 1
    max_connections: int = 1000
    current_connections: int = 0
    total_requests: int = 0
    failed_requests: int = 0
    avg_response_time: float = 0.0
    last_health_check: float = 0
    is_healthy: bool = True

    @property
    def connection_utilization(self) -> float:
        return self.current_connections / self.max_connections

    @property
    def error_rate(self) -> float:
        if self.total_requests == 0:
            return 0.0
        return self.failed_requests / self.total_requests

    @property
    def address(self) -> str:
        return f"{self.host}:{self.port}"

class LoadBalancer(ABC):
    """è² è¼‰å¹³è¡¡å™¨æŠ½è±¡åŸºé¡"""

    def __init__(self, nodes: List[ServerNode]):
        self.nodes = nodes
        self.current_index = 0

    @abstractmethod
    async def select_node(self, request_context: Dict[str, Any] = None) -> Optional[ServerNode]:
        """é¸æ“‡æœå‹™ç¯€é»"""
        pass

    async def get_healthy_nodes(self) -> List[ServerNode]:
        """å–å¾—å¥åº·çš„ç¯€é»"""
        return [node for node in self.nodes if node.is_healthy]

class RoundRobinBalancer(LoadBalancer):
    """è¼ªè©¢è² è¼‰å¹³è¡¡å™¨"""

    async def select_node(self, request_context: Dict[str, Any] = None) -> Optional[ServerNode]:
        healthy_nodes = await self.get_healthy_nodes()
        if not healthy_nodes:
            return None

        # è¼ªè©¢é¸æ“‡
        node = healthy_nodes[self.current_index % len(healthy_nodes)]
        self.current_index += 1
        return node

class WeightedRoundRobinBalancer(LoadBalancer):
    """åŠ æ¬Šè¼ªè©¢è² è¼‰å¹³è¡¡å™¨"""

    def __init__(self, nodes: List[ServerNode]):
        super().__init__(nodes)
        self.weighted_list = self._build_weighted_list()

    def _build_weighted_list(self) -> List[ServerNode]:
        """å»ºç«‹åŠ æ¬Šåˆ—è¡¨"""
        weighted_list = []
        for node in self.nodes:
            weighted_list.extend([node] * node.weight)
        return weighted_list

    async def select_node(self, request_context: Dict[str, Any] = None) -> Optional[ServerNode]:
        healthy_weighted = [node for node in self.weighted_list if node.is_healthy]
        if not healthy_weighted:
            return None

        node = healthy_weighted[self.current_index % len(healthy_weighted)]
        self.current_index += 1
        return node

class LeastConnectionsBalancer(LoadBalancer):
    """æœ€å°‘é€£æ¥æ•¸è² è¼‰å¹³è¡¡å™¨"""

    async def select_node(self, request_context: Dict[str, Any] = None) -> Optional[ServerNode]:
        healthy_nodes = await self.get_healthy_nodes()
        if not healthy_nodes:
            return None

        # é¸æ“‡é€£æ¥æ•¸æœ€å°‘çš„ç¯€é»
        return min(healthy_nodes, key=lambda node: node.current_connections)

class LeastResponseTimeBalancer(LoadBalancer):
    """æœ€çŸ­éŸ¿æ‡‰æ™‚é–“è² è¼‰å¹³è¡¡å™¨"""

    async def select_node(self, request_context: Dict[str, Any] = None) -> Optional[ServerNode]:
        healthy_nodes = await self.get_healthy_nodes()
        if not healthy_nodes:
            return None

        # é¸æ“‡å¹³å‡éŸ¿æ‡‰æ™‚é–“æœ€çŸ­çš„ç¯€é»
        return min(healthy_nodes, key=lambda node: node.avg_response_time)

class ConsistentHashBalancer(LoadBalancer):
    """ä¸€è‡´æ€§é›œæ¹Šè² è¼‰å¹³è¡¡å™¨"""

    def __init__(self, nodes: List[ServerNode], virtual_nodes: int = 150):
        super().__init__(nodes)
        self.virtual_nodes = virtual_nodes
        self.ring = self._build_hash_ring()

    def _build_hash_ring(self) -> Dict[int, ServerNode]:
        """å»ºç«‹é›œæ¹Šç’°"""
        ring = {}
        for node in self.nodes:
            for i in range(self.virtual_nodes):
                virtual_key = f"{node.id}:{i}"
                hash_value = hash(virtual_key) % (2**32)
                ring[hash_value] = node
        return ring

    async def select_node(self, request_context: Dict[str, Any] = None) -> Optional[ServerNode]:
        if not self.ring:
            return None

        # ä½¿ç”¨è«‹æ±‚çš„æŸå€‹æ¨™è­˜ç¬¦ä½œç‚ºé›œæ¹Šéµ
        hash_key = request_context.get("user_id", "") if request_context else ""
        hash_value = hash(hash_key) % (2**32)

        # åœ¨ç’°ä¸Šæ‰¾åˆ°ç¬¬ä¸€å€‹å¤§æ–¼ç­‰æ–¼ hash_value çš„ç¯€é»
        for ring_key in sorted(self.ring.keys()):
            if ring_key >= hash_value:
                node = self.ring[ring_key]
                if node.is_healthy:
                    return node

        # å¦‚æœæ²’æ‰¾åˆ°ï¼Œè¿”å›ç’°ä¸Šç¬¬ä¸€å€‹å¥åº·ç¯€é»
        for ring_key in sorted(self.ring.keys()):
            node = self.ring[ring_key]
            if node.is_healthy:
                return node

        return None

class SmartLoadBalancer:
    """æ™ºèƒ½è² è¼‰å¹³è¡¡å™¨"""

    def __init__(self, nodes: List[ServerNode]):
        self.nodes = {node.id: node for node in nodes}
        self.balancers = {
            BalancingStrategy.ROUND_ROBIN: RoundRobinBalancer(nodes),
            BalancingStrategy.WEIGHTED_ROUND_ROBIN: WeightedRoundRobinBalancer(nodes),
            BalancingStrategy.LEAST_CONNECTIONS: LeastConnectionsBalancer(nodes),
            BalancingStrategy.LEAST_RESPONSE_TIME: LeastResponseTimeBalancer(nodes),
            BalancingStrategy.CONSISTENT_HASH: ConsistentHashBalancer(nodes),
        }
        self.current_strategy = BalancingStrategy.ROUND_ROBIN
        self.strategy_performance = {strategy: {"requests": 0, "total_response_time": 0}
                                   for strategy in BalancingStrategy}

    async def select_node(self, request_context: Dict[str, Any] = None) -> Optional[ServerNode]:
        """æ™ºèƒ½é¸æ“‡ç¯€é»"""
        # æ ¹æ“šç•¶å‰è² è¼‰æƒ…æ³é¸æ“‡æœ€ä½³ç­–ç•¥
        optimal_strategy = await self._choose_optimal_strategy()

        if optimal_strategy != self.current_strategy:
            logger.info(f"Switching load balancing strategy from {self.current_strategy} to {optimal_strategy}")
            self.current_strategy = optimal_strategy

        balancer = self.balancers[self.current_strategy]
        return await balancer.select_node(request_context)

    async def _choose_optimal_strategy(self) -> BalancingStrategy:
        """é¸æ“‡æœ€ä½³ç­–ç•¥"""
        # åˆ†æç•¶å‰ç³»çµ±ç‹€æ…‹
        system_stats = await self._analyze_system_state()

        # é«˜è² è¼‰æƒ…æ³ä¸‹ä½¿ç”¨æœ€å°‘é€£æ¥æ•¸
        if system_stats["avg_connection_utilization"] > 0.8:
            return BalancingStrategy.LEAST_CONNECTIONS

        # éŸ¿æ‡‰æ™‚é–“å·®ç•°å¤§æ™‚ä½¿ç”¨æœ€çŸ­éŸ¿æ‡‰æ™‚é–“
        if system_stats["response_time_variance"] > 1000:  # 1ç§’å·®ç•°
            return BalancingStrategy.LEAST_RESPONSE_TIME

        # éœ€è¦æœƒè©±è¦ªå’Œæ€§æ™‚ä½¿ç”¨ä¸€è‡´æ€§é›œæ¹Š
        if system_stats["requires_session_affinity"]:
            return BalancingStrategy.CONSISTENT_HASH

        # é»˜èªä½¿ç”¨åŠ æ¬Šè¼ªè©¢
        return BalancingStrategy.WEIGHTED_ROUND_ROBIN

    async def _analyze_system_state(self) -> Dict[str, Any]:
        """åˆ†æç³»çµ±ç‹€æ…‹"""
        healthy_nodes = [node for node in self.nodes.values() if node.is_healthy]

        if not healthy_nodes:
            return {"avg_connection_utilization": 0, "response_time_variance": 0, "requires_session_affinity": False}

        # å¹³å‡é€£æ¥åˆ©ç”¨ç‡
        avg_utilization = sum(node.connection_utilization for node in healthy_nodes) / len(healthy_nodes)

        # éŸ¿æ‡‰æ™‚é–“è®Šç•°æ•¸
        response_times = [node.avg_response_time for node in healthy_nodes]
        avg_response_time = sum(response_times) / len(response_times)
        variance = sum((rt - avg_response_time) ** 2 for rt in response_times) / len(response_times)

        return {
            "avg_connection_utilization": avg_utilization,
            "response_time_variance": variance,
            "requires_session_affinity": False  # å¯æ ¹æ“šè«‹æ±‚é¡å‹åˆ¤æ–·
        }

    async def update_node_stats(self, node_id: str, response_time: float, success: bool):
        """æ›´æ–°ç¯€é»çµ±è¨ˆ"""
        if node_id not in self.nodes:
            return

        node = self.nodes[node_id]
        node.total_requests += 1

        if success:
            # æ›´æ–°å¹³å‡éŸ¿æ‡‰æ™‚é–“
            node.avg_response_time = (
                (node.avg_response_time * (node.total_requests - 1) + response_time) /
                node.total_requests
            )
        else:
            node.failed_requests += 1

        # æ›´æ–°ç­–ç•¥æ•ˆèƒ½çµ±è¨ˆ
        strategy_stats = self.strategy_performance[self.current_strategy]
        strategy_stats["requests"] += 1
        strategy_stats["total_response_time"] += response_time

class HealthChecker:
    """å¥åº·æª¢æŸ¥å™¨"""

    def __init__(self, load_balancer: SmartLoadBalancer, check_interval: int = 30):
        self.load_balancer = load_balancer
        self.check_interval = check_interval
        self.running = False

    async def start(self):
        """é–‹å§‹å¥åº·æª¢æŸ¥"""
        self.running = True
        while self.running:
            await self._check_all_nodes()
            await asyncio.sleep(self.check_interval)

    async def stop(self):
        """åœæ­¢å¥åº·æª¢æŸ¥"""
        self.running = False

    async def _check_all_nodes(self):
        """æª¢æŸ¥æ‰€æœ‰ç¯€é»å¥åº·ç‹€æ…‹"""
        tasks = []
        for node in self.load_balancer.nodes.values():
            task = asyncio.create_task(self._check_node_health(node))
            tasks.append(task)

        await asyncio.gather(*tasks, return_exceptions=True)

    async def _check_node_health(self, node: ServerNode):
        """æª¢æŸ¥å–®å€‹ç¯€é»å¥åº·ç‹€æ…‹"""
        try:
            # ç™¼é€å¥åº·æª¢æŸ¥è«‹æ±‚
            start_time = time.time()

            import httpx
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"http://{node.address}/health")

            response_time = time.time() - start_time

            # æ›´æ–°å¥åº·ç‹€æ…‹
            if response.status_code == 200:
                node.is_healthy = True
                node.last_health_check = time.time()

                # æ›´æ–°éŸ¿æ‡‰æ™‚é–“
                if node.total_requests == 0:
                    node.avg_response_time = response_time
                else:
                    node.avg_response_time = (
                        (node.avg_response_time * 0.9) + (response_time * 0.1)
                    )
            else:
                node.is_healthy = False

        except Exception as e:
            logger.error(f"Health check failed for node {node.id}", error=str(e))
            node.is_healthy = False

class AutoScaler:
    """è‡ªå‹•æ“´å±•å™¨"""

    def __init__(self, load_balancer: SmartLoadBalancer):
        self.load_balancer = load_balancer
        self.scaling_rules = self._load_scaling_rules()

    def _load_scaling_rules(self) -> Dict[str, Any]:
        """è¼‰å…¥æ“´å±•è¦å‰‡"""
        return {
            "scale_out_threshold": 0.8,  # CPU/é€£æ¥åˆ©ç”¨ç‡é–¾å€¼
            "scale_in_threshold": 0.3,
            "min_instances": 2,
            "max_instances": 10,
            "scale_out_cooldown": 300,  # 5åˆ†é˜
            "scale_in_cooldown": 600,   # 10åˆ†é˜
        }

    async def evaluate_scaling(self):
        """è©•ä¼°æ˜¯å¦éœ€è¦æ“´å±•"""
        current_load = await self._calculate_system_load()

        if await self._should_scale_out(current_load):
            await self._scale_out()
        elif await self._should_scale_in(current_load):
            await self._scale_in()

    async def _calculate_system_load(self) -> float:
        """è¨ˆç®—ç³»çµ±è² è¼‰"""
        healthy_nodes = [node for node in self.load_balancer.nodes.values() if node.is_healthy]

        if not healthy_nodes:
            return 1.0

        total_utilization = sum(node.connection_utilization for node in healthy_nodes)
        return total_utilization / len(healthy_nodes)

    async def _should_scale_out(self, current_load: float) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²æ©«å‘æ“´å±•"""
        return (
            current_load > self.scaling_rules["scale_out_threshold"] and
            len(self.load_balancer.nodes) < self.scaling_rules["max_instances"]
        )

    async def _should_scale_in(self, current_load: float) -> bool:
        """åˆ¤æ–·æ˜¯å¦æ‡‰è©²ç¸®æ¸›"""
        return (
            current_load < self.scaling_rules["scale_in_threshold"] and
            len(self.load_balancer.nodes) > self.scaling_rules["min_instances"]
        )

    async def _scale_out(self):
        """æ©«å‘æ“´å±•"""
        logger.info("Scaling out - adding new instance")
        # å¯¦éš›å¯¦ä½œæœƒèª¿ç”¨å®¹å™¨ç·¨æ’ç³»çµ±ï¼ˆå¦‚ Kubernetesï¼‰
        # é€™è£¡ç°¡åŒ–ç‚ºæ·»åŠ æ–°ç¯€é»çš„é‚è¼¯
        new_node = await self._create_new_instance()
        if new_node:
            self.load_balancer.nodes[new_node.id] = new_node
            logger.info(f"New instance added: {new_node.id}")

    async def _scale_in(self):
        """ç¸®æ¸›"""
        logger.info("Scaling in - removing instance")
        # é¸æ“‡è² è¼‰æœ€ä½çš„ç¯€é»ç§»é™¤
        node_to_remove = min(
            self.load_balancer.nodes.values(),
            key=lambda n: n.current_connections
        )

        # å„ªé›…é—œé–‰
        await self._graceful_shutdown(node_to_remove)
        del self.load_balancer.nodes[node_to_remove.id]
        logger.info(f"Instance removed: {node_to_remove.id}")

    async def _create_new_instance(self) -> Optional[ServerNode]:
        """å»ºç«‹æ–°å¯¦ä¾‹ï¼ˆéœ€è¦èˆ‡å®¹å™¨ç·¨æ’ç³»çµ±æ•´åˆï¼‰"""
        # é€™è£¡æ‡‰è©²èª¿ç”¨ Kubernetes API æˆ–å…¶ä»–ç·¨æ’å·¥å…·
        pass

    async def _graceful_shutdown(self, node: ServerNode):
        """å„ªé›…é—œé–‰ç¯€é»"""
        # åœæ­¢æ¥æ”¶æ–°è«‹æ±‚
        node.is_healthy = False

        # ç­‰å¾…ç¾æœ‰é€£æ¥å®Œæˆ
        timeout = 30  # 30ç§’è¶…æ™‚
        start_time = time.time()

        while node.current_connections > 0 and (time.time() - start_time) < timeout:
            await asyncio.sleep(1)

        logger.info(f"Node {node.id} gracefully shut down")
```

---

## ğŸ“ˆ æ•ˆèƒ½ç›£æ§èˆ‡å‘Šè­¦

### å³æ™‚æ•ˆèƒ½ç›£æ§ç³»çµ±

```python
# app/monitoring/performance_monitor.py
class PerformanceMonitor:
    """å³æ™‚æ•ˆèƒ½ç›£æ§ç³»çµ±"""

    def __init__(self):
        self.metrics_collector = PrometheusMetrics()
        self.alert_manager = AlertManager()
        self.performance_thresholds = self._load_thresholds()

    def _load_thresholds(self) -> Dict[str, float]:
        """è¼‰å…¥æ•ˆèƒ½é–¾å€¼"""
        return {
            "response_time_warning": 1.0,    # 1ç§’
            "response_time_critical": 5.0,   # 5ç§’
            "memory_usage_warning": 80.0,    # 80%
            "memory_usage_critical": 95.0,   # 95%
            "cpu_usage_warning": 70.0,       # 70%
            "cpu_usage_critical": 90.0,      # 90%
            "error_rate_warning": 0.05,      # 5%
            "error_rate_critical": 0.10,     # 10%
        }

    async def start_monitoring(self):
        """é–‹å§‹æ•ˆèƒ½ç›£æ§"""
        await asyncio.gather(
            self._monitor_response_times(),
            self._monitor_system_resources(),
            self._monitor_application_metrics(),
            self._monitor_database_performance()
        )

    async def _monitor_response_times(self):
        """ç›£æ§éŸ¿æ‡‰æ™‚é–“"""
        while True:
            try:
                # æ”¶é›†æœ€è¿‘ 5 åˆ†é˜çš„éŸ¿æ‡‰æ™‚é–“
                recent_response_times = await self._get_recent_response_times()

                if recent_response_times:
                    avg_response_time = sum(recent_response_times) / len(recent_response_times)
                    p95_response_time = self._calculate_percentile(recent_response_times, 0.95)

                    # è¨˜éŒ„æŒ‡æ¨™
                    self.metrics_collector.gauge("app_response_time_avg", avg_response_time)
                    self.metrics_collector.gauge("app_response_time_p95", p95_response_time)

                    # æª¢æŸ¥å‘Šè­¦
                    if p95_response_time > self.performance_thresholds["response_time_critical"]:
                        await self.alert_manager.send_alert(
                            severity="critical",
                            title="High Response Time",
                            description=f"95th percentile response time: {p95_response_time:.2f}s"
                        )
                    elif avg_response_time > self.performance_thresholds["response_time_warning"]:
                        await self.alert_manager.send_alert(
                            severity="warning",
                            title="Elevated Response Time",
                            description=f"Average response time: {avg_response_time:.2f}s"
                        )

                await asyncio.sleep(60)  # æ¯åˆ†é˜æª¢æŸ¥

            except Exception as e:
                logger.error("Response time monitoring failed", error=str(e))
                await asyncio.sleep(60)

    async def _monitor_system_resources(self):
        """ç›£æ§ç³»çµ±è³‡æº"""
        import psutil

        while True:
            try:
                # CPU ä½¿ç”¨ç‡
                cpu_percent = psutil.cpu_percent(interval=1)
                self.metrics_collector.gauge("system_cpu_usage", cpu_percent)

                # è¨˜æ†¶é«”ä½¿ç”¨ç‡
                memory = psutil.virtual_memory()
                memory_percent = memory.percent
                self.metrics_collector.gauge("system_memory_usage", memory_percent)

                # ç£ç¢Ÿ I/O
                disk_io = psutil.disk_io_counters()
                if disk_io:
                    self.metrics_collector.gauge("system_disk_read_bytes", disk_io.read_bytes)
                    self.metrics_collector.gauge("system_disk_write_bytes", disk_io.write_bytes)

                # ç¶²è·¯ I/O
                network_io = psutil.net_io_counters()
                if network_io:
                    self.metrics_collector.gauge("system_network_sent_bytes", network_io.bytes_sent)
                    self.metrics_collector.gauge("system_network_recv_bytes", network_io.bytes_recv)

                # æª¢æŸ¥å‘Šè­¦
                if memory_percent > self.performance_thresholds["memory_usage_critical"]:
                    await self.alert_manager.send_alert(
                        severity="critical",
                        title="High Memory Usage",
                        description=f"Memory usage: {memory_percent:.1f}%"
                    )

                if cpu_percent > self.performance_thresholds["cpu_usage_critical"]:
                    await self.alert_manager.send_alert(
                        severity="critical",
                        title="High CPU Usage",
                        description=f"CPU usage: {cpu_percent:.1f}%"
                    )

                await asyncio.sleep(30)  # æ¯ 30 ç§’æª¢æŸ¥

            except Exception as e:
                logger.error("System resource monitoring failed", error=str(e))
                await asyncio.sleep(60)

    def _calculate_percentile(self, data: List[float], percentile: float) -> float:
        """è¨ˆç®—ç™¾åˆ†ä½æ•¸"""
        sorted_data = sorted(data)
        k = (len(sorted_data) - 1) * percentile
        f = int(k)
        c = k - f

        if f == len(sorted_data) - 1:
            return sorted_data[f]

        return sorted_data[f] * (1 - c) + sorted_data[f + 1] * c

class PerformanceOptimizer:
    """æ•ˆèƒ½å„ªåŒ–å™¨"""

    def __init__(self):
        self.optimization_rules = self._load_optimization_rules()
        self.current_optimizations = set()

    def _load_optimization_rules(self) -> List[Dict[str, Any]]:
        """è¼‰å…¥å„ªåŒ–è¦å‰‡"""
        return [
            {
                "name": "enable_gzip_compression",
                "condition": lambda metrics: metrics.get("avg_response_size", 0) > 10000,  # 10KB
                "action": self._enable_gzip_compression,
                "priority": 1
            },
            {
                "name": "increase_connection_pool",
                "condition": lambda metrics: metrics.get("db_connection_utilization", 0) > 0.8,
                "action": self._increase_connection_pool,
                "priority": 2
            },
            {
                "name": "enable_query_cache",
                "condition": lambda metrics: metrics.get("db_query_time_avg", 0) > 100,  # 100ms
                "action": self._enable_query_cache,
                "priority": 3
            },
            {
                "name": "optimize_memory_usage",
                "condition": lambda metrics: metrics.get("memory_usage", 0) > 85,
                "action": self._optimize_memory_usage,
                "priority": 4
            }
        ]

    async def evaluate_optimizations(self, metrics: Dict[str, float]):
        """è©•ä¼°å’Œæ‡‰ç”¨å„ªåŒ–"""
        applicable_optimizations = []

        for rule in self.optimization_rules:
            if rule["condition"](metrics) and rule["name"] not in self.current_optimizations:
                applicable_optimizations.append(rule)

        # æŒ‰å„ªå…ˆç´šæ’åº
        applicable_optimizations.sort(key=lambda x: x["priority"])

        for rule in applicable_optimizations:
            try:
                await rule["action"]()
                self.current_optimizations.add(rule["name"])
                logger.info(f"Applied optimization: {rule['name']}")
            except Exception as e:
                logger.error(f"Optimization failed: {rule['name']}", error=str(e))

    async def _enable_gzip_compression(self):
        """å•Ÿç”¨ GZIP å£“ç¸®"""
        # å‹•æ…‹èª¿æ•´ GZIP è¨­å®š
        logger.info("Enabling GZIP compression for responses > 1KB")

    async def _increase_connection_pool(self):
        """å¢åŠ é€£æ¥æ± å¤§å°"""
        # å‹•æ…‹å¢åŠ è³‡æ–™åº«é€£æ¥æ± å¤§å°
        logger.info("Increasing database connection pool size")

    async def _enable_query_cache(self):
        """å•Ÿç”¨æŸ¥è©¢å¿«å–"""
        # å•Ÿç”¨æ›´ç©æ¥µçš„æŸ¥è©¢å¿«å–
        logger.info("Enabling aggressive query caching")

    async def _optimize_memory_usage(self):
        """å„ªåŒ–è¨˜æ†¶é«”ä½¿ç”¨"""
        # è§¸ç™¼åƒåœ¾å›æ”¶
        import gc
        gc.collect()
        logger.info("Triggered garbage collection to optimize memory")

class PerformanceAnalyzer:
    """æ•ˆèƒ½åˆ†æå™¨"""

    async def generate_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ•ˆèƒ½åˆ†æå ±å‘Š"""
        report = {
            "timestamp": datetime.utcnow().isoformat(),
            "response_time_analysis": await self._analyze_response_times(),
            "resource_usage_analysis": await self._analyze_resource_usage(),
            "bottleneck_analysis": await self._identify_bottlenecks(),
            "optimization_recommendations": await self._generate_recommendations()
        }

        return report

    async def _analyze_response_times(self) -> Dict[str, Any]:
        """åˆ†æéŸ¿æ‡‰æ™‚é–“"""
        # åˆ†ææœ€è¿‘ 1 å°æ™‚çš„éŸ¿æ‡‰æ™‚é–“æ•¸æ“š
        response_times = await self._get_response_time_history(hours=1)

        if not response_times:
            return {}

        return {
            "average": sum(response_times) / len(response_times),
            "median": self._calculate_percentile(response_times, 0.5),
            "p95": self._calculate_percentile(response_times, 0.95),
            "p99": self._calculate_percentile(response_times, 0.99),
            "max": max(response_times),
            "min": min(response_times),
            "trend": await self._calculate_trend(response_times)
        }

    async def _identify_bottlenecks(self) -> List[Dict[str, Any]]:
        """è­˜åˆ¥æ•ˆèƒ½ç“¶é ¸"""
        bottlenecks = []

        # åˆ†æè³‡æ–™åº«æ•ˆèƒ½
        db_bottlenecks = await self._analyze_database_bottlenecks()
        bottlenecks.extend(db_bottlenecks)

        # åˆ†æç¶²è·¯æ•ˆèƒ½
        network_bottlenecks = await self._analyze_network_bottlenecks()
        bottlenecks.extend(network_bottlenecks)

        # åˆ†ææ‡‰ç”¨å±¤æ•ˆèƒ½
        app_bottlenecks = await self._analyze_application_bottlenecks()
        bottlenecks.extend(app_bottlenecks)

        return sorted(bottlenecks, key=lambda x: x["impact_score"], reverse=True)

    async def _generate_recommendations(self) -> List[str]:
        """ç”Ÿæˆå„ªåŒ–å»ºè­°"""
        recommendations = []

        # åŸºæ–¼åˆ†æçµæœç”Ÿæˆå»ºè­°
        bottlenecks = await self._identify_bottlenecks()

        for bottleneck in bottlenecks[:5]:  # å‰ 5 å€‹æœ€åš´é‡çš„ç“¶é ¸
            if bottleneck["type"] == "database":
                recommendations.append(f"å„ªåŒ–è³‡æ–™åº«æŸ¥è©¢: {bottleneck['description']}")
            elif bottleneck["type"] == "memory":
                recommendations.append(f"è¨˜æ†¶é«”å„ªåŒ–: {bottleneck['description']}")
            elif bottleneck["type"] == "cpu":
                recommendations.append(f"CPU å„ªåŒ–: {bottleneck['description']}")
            elif bottleneck["type"] == "network":
                recommendations.append(f"ç¶²è·¯å„ªåŒ–: {bottleneck['description']}")

        return recommendations
```

---

## ğŸ’¡ å­¸ç¿’ç­†è¨˜å€

### ğŸ¤” æˆ‘çš„ç†è§£
```
æ•ˆèƒ½åˆ†æçš„ç³»çµ±æ€§æ–¹æ³•ï¼š

FastAPI æ•ˆèƒ½å„ªåŒ–çš„é—œéµé»ï¼š

å¿«å–ç­–ç•¥çš„è¨­è¨ˆåŸå‰‡ï¼š

è² è¼‰å¹³è¡¡çš„é¸æ“‡è€ƒé‡ï¼š
```

### âš¡ å¯¦è¸å¿ƒå¾—
```
æ•ˆèƒ½èª¿å„ªéç¨‹ä¸­çš„æŒ‘æˆ°ï¼š

æœ€æœ‰æ•ˆçš„å„ªåŒ–æŠ€è¡“ï¼š

ç›£æ§ç³»çµ±çš„é‡è¦æ€§ï¼š

æ“´å±•ç­–ç•¥çš„è€ƒé‡å› ç´ ï¼š
```

### ğŸš€ é€²éšæ€è€ƒ
```
ç¾ä»£æ‡‰ç”¨çš„æ•ˆèƒ½æŒ‘æˆ°ï¼š

é›²åŸç”Ÿç’°å¢ƒçš„å„ªåŒ–ç­–ç•¥ï¼š

AI åœ¨æ•ˆèƒ½å„ªåŒ–ä¸­çš„æ‡‰ç”¨ï¼š

æœªä¾†æ•ˆèƒ½å„ªåŒ–æŠ€è¡“è¶¨å‹¢ï¼š
```